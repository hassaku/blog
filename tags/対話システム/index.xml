<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>対話システム on hassaku&#39;s blog</title>
    <link>http://blog.hassaku-labs.com/tags/%E5%AF%BE%E8%A9%B1%E3%82%B7%E3%82%B9%E3%83%86%E3%83%A0/</link>
    <description>Recent content in 対話システム on hassaku&#39;s blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>ja</language>
    <lastBuildDate>Sun, 26 Feb 2017 02:27:30 +0900</lastBuildDate>
    
	<atom:link href="http://blog.hassaku-labs.com/tags/%E5%AF%BE%E8%A9%B1%E3%82%B7%E3%82%B9%E3%83%86%E3%83%A0/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>学習済みword2vecモデルを調べてみた</title>
      <link>http://blog.hassaku-labs.com/post/pretrained-word2vec/</link>
      <pubDate>Sun, 26 Feb 2017 02:27:30 +0900</pubDate>
      
      <guid>http://blog.hassaku-labs.com/post/pretrained-word2vec/</guid>
      <description>日本語の自然言語処理で分散表現を使おうと思った場合、まず頭に浮かぶのはword2vecだと思います。 特に分散表現自体の精度とかには興味がなく、それを使った対話システムを作りたいだけだったりするのであれば、 データクレンジングや学習には結構時間もかかるので、学習済みの公開モデルを使わせていただくのが手っ取り早そうです。
(単語ベクトルの準備に手間取り、モチベーション低下に繋がる悪い例：対話システムを作りたい！【準備編１】)
調べてみると、よく出来ていそうな公開モデルを２つ見つけたので、その利用方法と気になるベクトル次元数と単語数を調べてみました。 なお、どちらもWikipedia日本語版を学習元にしているようです。
word2vecを使うには、以下のバージョンのgensimを利用します。
$ pip freeze | grep gensim gensim==1.0.0  白ヤギコーポレーションのモデル word2vecの学習済み日本語モデルを公開します
&amp;gt;&amp;gt;&amp;gt; from gensim.models import word2vec &amp;gt;&amp;gt;&amp;gt; model = word2vec.Word2Vec.load(&#39;./shiroyagi/word2vec.gensim.model&#39;) &amp;gt;&amp;gt;&amp;gt; model[u&#39;ニュース&#39;].shape (50,) &amp;gt;&amp;gt;&amp;gt; model.corpus_count 1046708  東北大学 乾・岡崎研究室のモデル 日本語 Wikipedia エンティティベクトル
&amp;gt;&amp;gt;&amp;gt; from gensim.models import KeyedVectors &amp;gt;&amp;gt;&amp;gt; model = KeyedVectors.load_word2vec_format(&#39;./tohoku_entity_vector/entity_vector.model.bin&#39;, binary=True) &amp;gt;&amp;gt;&amp;gt; model[u&#39;ニュース&#39;].shape (200,) &amp;gt;&amp;gt;&amp;gt; len(model.vocab) 1005367  gensimが1.0.0未満の場合（ただしシロヤギさんのモデルは1.0.0以上が必要のようです） &amp;gt;&amp;gt;&amp;gt; from gensim.models import word2vec &amp;gt;&amp;gt;&amp;gt; model = word2vec.Word2Vec.load_word2vec_format(&#39;./tohoku_entity_vector/entity_vector.model.bin&#39;, binary=True)  （追記）Facebookの学習済みFastTextモデル 後日、FacebookのFastTextのレポジトリでも、日本語Wikipediaの分散表現モデルが公開されているのを見つけました。 ボキャブラリ数は少ないみたいですが、ベクトルの次元数が一番大きいです。 もしかすると、ボキャブラリ数に関しては、容量の観点から、不要な用語を除去するなど、うまくデータクレンジングされた結果なのかもしれません。 利用時には要調査です。</description>
    </item>
    
    <item>
      <title>対話システムを作りたい！（実際はWikipediaのデータを取得して単語ベクトルを学習するまで）</title>
      <link>http://blog.hassaku-labs.com/post/dialogue_system1/</link>
      <pubDate>Fri, 01 Apr 2016 02:26:21 +0900</pubDate>
      
      <guid>http://blog.hassaku-labs.com/post/dialogue_system1/</guid>
      <description>2016年はVRとか流行りそうで、仮想空間での生活を妄想してしまう今日このごろ。 でも、今のままだとNPCがちゃんと自然に会話してくれない気がして微妙なんですよね。。。そこで、既存技術の延長で、どれくらいの日本語対話が可能か、ちょっと自分でも作ってみたくなりました。
自然言語処理をちゃんと勉強したことはないけれど、脳型情報処理アプローチでいくとしたら、結局はベクトル時系列データの処理なのかな？って思います。とりあえず、色々試してみましょう。
たぶん、進め方はこんな感じ。
 単語のベクトルデータ化（言語コーパス？） 対話データのベクトル時系列データ化（対話コーパス？） 会話時系列データにおける応答時系列データの予測学習 学習結果を用いた対話システム構築  というわけで、今回は１の言語コーパス作成について。単語を入力とし、N次元ベクトルに変換することを目標。似たような単語は近くに配置されるような変換が好ましい（分散表現だ！）。
言語コーパスをWikipediaの記事から作成 wikipediaの日本語記事をダウンロード $ curl -O http://dumps.wikimedia.org/jawiki/latest/jawiki-latest-pages-articles.xml.bz  mecabをインストール 単語の分かち書きへ変換するためのツールです。macならbrewでインストール可能。
$ brew install mecab $ brew install mecab-ipadic  新語用辞書をインストール 最近の単語は、brewでインストールされた辞書には含まれていないので、新語に対応した辞書に更新します。
$ git clone --depth 1 git@github.com:neologd/mecab-ipadic-neologd.git $ cd mecab-ipadic-neologd/ $ ./bin/install-mecab-ipadic-neologd -n # 辞書updateも同じコマンド $ echo `mecab-config --dicdir`&amp;quot;/mecab-ipadic-neologd&amp;quot; # 実行時指定のパスを調べる /usr/local/lib/mecab/dic/mecab-ipadic-neologd  （参考）新御用辞書有無を確認 $ pip install mecab $ python In [1]: import MeCab In [2]: mecab_org = MeCab.Tagger(&amp;quot;-Owakati&amp;quot;) In [3]: mecab_new = MeCab.</description>
    </item>
    
  </channel>
</rss>