<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Ai on hassaku&#39;s blog</title>
    <link>http://blog.hassaku-labs.com/tags/ai/</link>
    <description>Recent content in Ai on hassaku&#39;s blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>ja</language>
    <lastBuildDate>Sun, 05 May 2019 10:00:00 +0900</lastBuildDate>
    <atom:link href="http://blog.hassaku-labs.com/tags/ai/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>機械学習技術を使った実際の研究開発プロジェクトについて</title>
      <link>http://blog.hassaku-labs.com/post/machine-learning-project/</link>
      <pubDate>Sun, 05 May 2019 10:00:00 +0900</pubDate>
      
      <guid>http://blog.hassaku-labs.com/post/machine-learning-project/</guid>
      <description>

&lt;p&gt;まずはざっくり箇条書き。多くを説明すべきところは、そのうち別の記事にするかも。
あとあくまでも、ある分野での機械学習案件における知見で、世の中一般に関して確立した見方はまだ無いと思われる。&lt;/p&gt;

&lt;h1 id=&#34;全体的な雰囲気:d69212eb41b9ab5639c2ccaa05b1fd75&#34;&gt;全体的な雰囲気&lt;/h1&gt;

&lt;p&gt;機械学習技術が必要になる仕事は全体の２割程度。よって、機械学習技術に精通していなくても活躍できる場面は多い。
むしろ、AutoMLや機械学習部分を自動化するようなフレームワークやツールが増えてきており、その他８割の方が今後は重要になるとも言える。
もちろん、その他作業を効率良く進めるためには、詳しいメンバーがいるに越したことはない。&lt;/p&gt;

&lt;h1 id=&#34;だいたいの流れ:d69212eb41b9ab5639c2ccaa05b1fd75&#34;&gt;だいたいの流れ&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;解こうとしている課題の理解

&lt;ul&gt;
&lt;li&gt;本当に機械学習必要としているのかも早めに議論が必要&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;データの理解

&lt;ul&gt;
&lt;li&gt;可視化とか色々して仮説を立てる準備を整える&lt;/li&gt;
&lt;li&gt;この時点でゴミデータの存在には気づいておくことが大事&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;仮説の検討

&lt;ul&gt;
&lt;li&gt;人がちょっと考えて解ける問題は、入出力前提が同じであれば、（たぶん）機械学習で解ける

&lt;ul&gt;
&lt;li&gt;例）加速度センサーのデータを見て、走っているか歩いているかを判定する&lt;/li&gt;
&lt;li&gt;データが十分にあればDeep Learningを試しても良いだろう&lt;/li&gt;
&lt;li&gt;データが少なかったり、なんとなくルールが見えているような場合は、適切な特徴量エンジニアリングと判別器で解けるだろう&lt;/li&gt;
&lt;li&gt;仮説検討時の思考過程を参考にすること&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;人が考えてもなんだか分からない場合

&lt;ul&gt;
&lt;li&gt;例）加速度センサーのデータを見て、病気かどうか判定する&lt;/li&gt;
&lt;li&gt;他のデータを使えるか相談したり、そもそも病気ではないものを判定するタスクに置き換えられないか検討するといいかも&lt;/li&gt;
&lt;li&gt;機械学習精度どうこうの前に、そもそも実現性の検証が必要となり、だいたいコスパ悪い

&lt;ul&gt;
&lt;li&gt;一方で実現できたときは、優位性が認められることになるので、挑戦することは否定しない（置かれている状況次第）&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;データセットの準備

&lt;ul&gt;
&lt;li&gt;学習用、検証用、評価用

&lt;ul&gt;
&lt;li&gt;検証用、評価用データの抽出には時間をかけて良い&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;データ不足や偏り

&lt;ul&gt;
&lt;li&gt;モデル選択・パラメータチューニング用に評価用データを使うことがないように十分注意すること

&lt;ul&gt;
&lt;li&gt;検証用で色々最適化した結果、評価用でも良い結果。というのが期待される進捗&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;解こうとしているタスクに対して適切かどうか、見極めがとても大事&lt;/li&gt;
&lt;li&gt;適当に進めて間違っていた場合、このあとのプロセスが全て台無しになるだろう

&lt;ul&gt;
&lt;li&gt;適当に選んでいわゆるleakageがあったりすると悲惨&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;評価尺度の定義

&lt;ul&gt;
&lt;li&gt;ビジネス課題との整合性確認&lt;/li&gt;
&lt;li&gt;単なるF値やAUCを追い求めたら良いのか？あるいは、Precisionを重視してほしいのか等&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;評価系の構築

&lt;ul&gt;
&lt;li&gt;適切なデータセットと評価尺度が用意できれば、ほぼ終わったも同然（？）&lt;/li&gt;
&lt;li&gt;このあとはここまで色々頑張ってきた準備に対する、収穫の時期となるだろう

&lt;ul&gt;
&lt;li&gt;変な話、機械学習技術について精通した人が、機械学習未満のここまでをちゃんと設計出来ていると良いかも&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;特徴抽出の検討

&lt;ul&gt;
&lt;li&gt;あとで色々組み合わせたり、除外したくなるので、特徴量毎に個別CSV化しておくなどしておくと良い&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;機械学習技術の検討

&lt;ul&gt;
&lt;li&gt;ルールベースやロジスティック回帰などシンプルな手法で早めにベースラインを用意&lt;/li&gt;
&lt;li&gt;楽しい ٩( &amp;lsquo;ω&amp;rsquo; )و ♬*&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;検討した手法の評価

&lt;ul&gt;
&lt;li&gt;並列化とか早めにして、試行錯誤をすばやくする仕組みが大事&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;評価結果の課題分析

&lt;ul&gt;
&lt;li&gt;underfitting

&lt;ul&gt;
&lt;li&gt;表現能力を上げる。Deep Learningだったら層や素子数、その他だったら特徴量増やしたり&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;overfitting

&lt;ul&gt;
&lt;li&gt;正則化とかデータ増やしたりとか&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;具体的にうまくいかないデータを見たり、クラスタリング・可視化してみたりして、仮説を立てることはここでも大事&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;システム統合

&lt;ul&gt;
&lt;li&gt;精度を犠牲にしてでも、処理速度を上げたりしないといけないかも&amp;hellip;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;統合後の逐次評価

&lt;ul&gt;
&lt;li&gt;未知データの出現や分布、特徴の傾向が変化するかもしれないので継続的な改善は必要&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;各プロセスで、なにか壁にぶつかったら、必要なところまで戻り、を繰り返すのが機械学習リサーチエンジニアの主な仕事（たぶん）。&lt;/p&gt;

&lt;h1 id=&#34;進捗が芳しくない状況でありそうなケース:d69212eb41b9ab5639c2ccaa05b1fd75&#34;&gt;進捗が芳しくない状況でありそうなケース&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;パラメータチューニングばかりやってる&lt;/li&gt;
&lt;li&gt;データクレンジングしてない&lt;/li&gt;
&lt;li&gt;課題を適切に分割してない&lt;/li&gt;
&lt;li&gt;解こうとしている問題のコスパが悪い&lt;/li&gt;
&lt;li&gt;仮説もってない&lt;/li&gt;
&lt;li&gt;トレードオフを行ったり来たりしてる&lt;/li&gt;
&lt;li&gt;etc&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;一時的なサポートメンバーに仕事を任せるために考えること:d69212eb41b9ab5639c2ccaa05b1fd75&#34;&gt;一時的なサポートメンバーに仕事を任せるために考えること&lt;/h1&gt;

&lt;p&gt;いま一番頭を悩ましている部分&amp;hellip;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;評価系まではきちんと作り込んでおく&lt;/li&gt;
&lt;li&gt;自走出来なそうなら、試行錯誤するための手順をある程度つけておく

&lt;ul&gt;
&lt;li&gt;任せられるタスク範囲が絞られるのでトレードオフ&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;手順化したようなところは自動化出来てしまったりするので悩ましかったりもする&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;進捗ミーティングとかで共有すべき内容:d69212eb41b9ab5639c2ccaa05b1fd75&#34;&gt;進捗ミーティングとかで共有すべき内容&lt;/h1&gt;

&lt;p&gt;やったことではなく、出来たことの説明を優先すべき&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;今回解決しようとした課題&lt;/li&gt;
&lt;li&gt;今回の課題が全体に占める割合&lt;/li&gt;
&lt;li&gt;解決方法&lt;/li&gt;
&lt;li&gt;結果

&lt;ul&gt;
&lt;li&gt;検討開始からの評価結果推移&lt;/li&gt;
&lt;li&gt;課題解決を確認できそうな代表的なグラフ&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;残課題&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;場合によっては活きてくるアドバンスドな-手法:d69212eb41b9ab5639c2ccaa05b1fd75&#34;&gt;場合によっては活きてくるアドバンスドな(?)手法&lt;/h1&gt;

&lt;p&gt;まずはトイタスクを用意して、原理検証することが大事&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;特権付き機械学習&lt;/li&gt;
&lt;li&gt;ロバスト最適化&lt;/li&gt;
&lt;li&gt;Semi-superised学習&lt;/li&gt;
&lt;li&gt;Positive-unlabeled学習&lt;/li&gt;
&lt;li&gt;共変量シフト適応&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;一般的なシステム開発プロジェクトと比べたときの難しさ:d69212eb41b9ab5639c2ccaa05b1fd75&#34;&gt;一般的なシステム開発プロジェクトと比べたときの難しさ&lt;/h1&gt;

&lt;p&gt;&lt;a href=&#34;https://medium.com/@l2k/why-are-machine-learning-projects-so-hard-to-manage-8e9b9cf49641&#34;&gt;https://medium.com/@l2k/why-are-machine-learning-projects-so-hard-to-manage-8e9b9cf49641&lt;/a&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;事前に難易度の見通しが付きづらい

&lt;ul&gt;
&lt;li&gt;その課題で90%の意味するところは？そもそも実現可能？70%でも十分では？&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;想定外に失敗することがある

&lt;ul&gt;
&lt;li&gt;突如としてデータの傾向が変わるなど（金融データとか）&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;十分なデータが必要

&lt;ul&gt;
&lt;li&gt;多ければ良いというわけではなく、適切なデータを集めなくてはならない&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;etc.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>人工知能実現のための要素技術アイデア</title>
      <link>http://blog.hassaku-labs.com/post/ai-ideas/</link>
      <pubDate>Thu, 02 May 2019 10:00:00 +0900</pubDate>
      
      <guid>http://blog.hassaku-labs.com/post/ai-ideas/</guid>
      <description>

&lt;p&gt;ただの夢想です。実現するには時間も考えも足りないので、忘れないうちに言語化しておきます。&lt;/p&gt;

&lt;h1 id=&#34;実現したいこと:781c8ffbbe578c698fac6a5653ee4ea2&#34;&gt;実現したいこと&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;コンセプト

&lt;ul&gt;
&lt;li&gt;心を感じるような人工的な知能を実現すること

&lt;ul&gt;
&lt;li&gt;条件反射ではない、なにか考えているような印象を与えてくれる&lt;/li&gt;
&lt;li&gt;常に学習し続けていて、変化を伴う&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;「疲れている」と伝えれば、それに共感してくれたり、自身のことを話してくれたりするイメージ&lt;/li&gt;
&lt;li&gt;知りたいことが返ってこなくても良い。むしろ何か見えないダイナミクスを感じさせてくれる挙動が大事&lt;/li&gt;
&lt;li&gt;知識の抽出だけではなく、知識の使い方をも自律的に発見する&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;シミュレーション

&lt;ul&gt;
&lt;li&gt;ユーザ（私）と体験をある程度共有出来るような環境&lt;/li&gt;
&lt;li&gt;グリッドワールドでも良い。その中に、時間や環境（温度や明るさ）、空腹感などの外的変動要素が含まれていること&lt;/li&gt;
&lt;li&gt;「今日は暑いね」と言えば、「暑いとは温度が比較的高いこと」など体験に基づいて内的に理解出来ていること&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;要素技術:781c8ffbbe578c698fac6a5653ee4ea2&#34;&gt;要素技術&lt;/h1&gt;

&lt;p&gt;情報表現も記憶も全ては動的であり、それを制御するための力学系を有すること&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;機構&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;基本は深層ニューラルネットのように徐々に抽象化されていく仕組み&lt;/li&gt;
&lt;li&gt;ただし層数は事前に決まらず、ニューロン同士の結合有無組み合わせ、再帰的な回路、時間発展によって、機能的に深層構造と同等の機能が動的に実現される&lt;/li&gt;
&lt;li&gt;結合の仕方を他のニューラルネットの状態によって制御出来れば、状況によって最適な構造（表現能力）を動的に用いることが出来る&lt;/li&gt;
&lt;li&gt;ある２つの情報が、同じカテゴリとして表現したいときもあれば、明確に区別したいときもあり、Attention機構（回路構造の指定）によって動的に制御される&lt;/li&gt;
&lt;li&gt;結合有無に関しては、MC Dropoutのようにある種のランダムを持たせて、いくらか学習が進んだあとに、エントロピー的計算により既知未知をある程度算出できる&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;学習方法&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;常に学習し続けても破綻しない&lt;/li&gt;
&lt;li&gt;誤差逆伝搬だと追加学習や並列学習に難があるので、Equilibrium Propagationのようなローカルな方法が望ましい&lt;/li&gt;
&lt;li&gt;誤差を求めるための正解値みたいなものは基本的に明示されず、自身の行動を決める上で、その情報表現が適切か不適切かを指標とする&lt;/li&gt;
&lt;li&gt;言い換えると、全ては時系列の予測学習であり、予測が出来るように学習していくことが基本的な戦略

&lt;ul&gt;
&lt;li&gt;時系列の先が同じであれば、表現は近くなるといった、CBOWとかskip-gramのイメージにも近い&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;記憶&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;大脳皮質の役割で、まだあまり議論されていないように思える&lt;/li&gt;
&lt;li&gt;hopfield型NNのようにアトラクタを有する力学系で実現される

&lt;ul&gt;
&lt;li&gt;追加学習をしても過去の記憶は壊れない。ただし、想起しづらくなることはある&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;AからBの予測を単に関数近似ではなく、神経力学系上の時間発展が可能とする連想記憶によって実現される

&lt;ul&gt;
&lt;li&gt;これによりA-&amp;gt;B、B-&amp;gt;Cを個別に学習しておくだけで、A-&amp;gt;Cの三段論法的な表現も自動的に獲得される&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;様々な角度の連想記憶は、力学系の部分空間の切り替えによって動的に制御される&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;探索&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;根底にあるのは欲求や感覚であり、持って生まれるものとする&lt;/li&gt;
&lt;li&gt;明示的なゴールは基本的に与えられない&lt;/li&gt;
&lt;li&gt;ご飯は食べれば食欲は満たされるポジティブな情報として、嫌なことは最終的に痛みに結びつきネガティブな情報として表現できる&lt;/li&gt;
&lt;li&gt;また、既知未知を表現できることで、知識欲を満たすように自律的な探索行動もできる&lt;/li&gt;
&lt;li&gt;そうやって、ネガティブなことを避けつつ、ポジティブなことを追い求めていく中で、徐々に外界情報が内部表現として整理されていく&lt;/li&gt;
&lt;li&gt;そうした中で、ユーザ（私）との共通認識みたいなものも生まれて、コミュニケーションも取れるようになるはず&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;書きかけであり、適宜追記するし、画像も用意する&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>ターミナル上でシンプルなグリッドワールド</title>
      <link>http://blog.hassaku-labs.com/post/grid-world/</link>
      <pubDate>Fri, 10 Aug 2018 10:00:00 +0900</pubDate>
      
      <guid>http://blog.hassaku-labs.com/post/grid-world/</guid>
      <description>&lt;p&gt;強化学習などでグリッドワールドを使いたいとき、gym-minigridとかpycolabがあるけど、色々いじる必要性もある場合、もっとシンプルなところからはじめたい。
また、リモートのVMインスタンス上などで気軽に動かしたいので、GUIとかも無しで、ターミナル上で動かしたい。&lt;/p&gt;

&lt;p&gt;以下のような感じで、cursesを使ってスクラッチで作っても別に難しいことはなかった。&lt;/p&gt;

&lt;p&gt;こんな感じのやつがターミナル上で動く。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://blog.hassaku-labs.com/images/post/grid-world.gif&#34; alt=&#34;grid-world&#34; /&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;import curses
import random
import time
from datetime import datetime

FIELD = [&#39;#################&#39;,
         &#39;#       #       #&#39;,
         &#39;#       #       #&#39;,
         &#39;#       #       #&#39;,
         &#39;#               #&#39;,
         &#39;#       #       #&#39;,
         &#39;######  #########&#39;,
         &#39;#       #       #&#39;,
         &#39;#       #       #&#39;,
         &#39;#               #&#39;,
         &#39;#       #       #&#39;,
         &#39;#       #       #&#39;,
         &#39;#################&#39;]


def draw(screen):
    for row, line in enumerate(FIELD):
        for col, tile in enumerate(line):
            screen.addch(row, col, tile)


def main():
    x = 10
    y = 10

    try:
        screen = curses.initscr()
        screen.nodelay(1)
        curses.curs_set(0)

        while(True):
            action = random.randint(1, 5)
            dx = 0
            dy = 0
            if action == 1:
                dy += 1
            elif action == 2:
                dy -= 1
            elif action == 3:
                dx += 1
            elif action == 4:
                dx -= 1
            elif action == 5:
                pass
            else:
                raise NotImplementedError()

            # check wall
            if FIELD[x + dx][y + dy] != &amp;quot;#&amp;quot;:
                x += dx
                y += dy

            screen.clear()
            draw(screen)
            screen.addch(x, y, &#39;+&#39;) # agent

            screen.addstr(0, 20, datetime.now().strftime(&amp;quot;%Y/%m/%d %H:%M:%S&amp;quot;))
            screen.addstr(1, 20, &#39;a:{} x:{} y:{}&#39;.format(action, x, y))
            screen.refresh()

            # quit
            if(screen.getch() == ord(&#39;q&#39;)):
                break

            time.sleep(0.2)

        curses.endwin()

    except:
        pass

    finally:
        curses.echo()
        curses.endwin()


if __name__ == &#39;__main__&#39;:
    main()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;もしエージェントとインタラクションしたいと思ったら、flaskとかでapi作って状態変えるのが良いと思う。やり方はまた別の機会に。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>単語ベクトルと全結合ニューラルネットワークによる単語連想記憶</title>
      <link>http://blog.hassaku-labs.com/post/associative-word-vector/</link>
      <pubDate>Sun, 15 Jul 2018 00:00:00 +0900</pubDate>
      
      <guid>http://blog.hassaku-labs.com/post/associative-word-vector/</guid>
      <description>

&lt;p&gt;自然言語処理にニューラルネットワークを適用する事例が増えている。
ここでは、従来の部分的に再帰結合をもつようなRNN(LSTM)といった構造ではなく、
(最近はあまり流行っていない）ホップフィールドモデルのような全結合構造のニューラルネットを用いて、
エネルギーポテンシャルの窪み、引き込み領域を有するアトラクタ空間に言語知識を記憶させることを目指す。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://blog.hassaku-labs.com/images/post/associative-word-vector/potential.png&#34; alt=&#34;potential&#34; /&gt;&lt;/p&gt;

&lt;p&gt;メリットとして、以下のようなことが挙げられる。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;追加学習が容易&lt;/li&gt;
&lt;li&gt;データを大量に学習しなくても汎化性能が高い&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;いくつか想定している内容のうち、今回は単語ベクトルのペアを連想記憶する基本的なタスクを検証する。&lt;/p&gt;

&lt;h1 id=&#34;検証用コード:92074d8141b5d0b357587f68abb31ec5&#34;&gt;検証用コード&lt;/h1&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/hassaku/associative-word-vector/tree/blog-1&#34;&gt;https://github.com/hassaku/associative-word-vector/tree/blog-1&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;検証内容:92074d8141b5d0b357587f68abb31ec5&#34;&gt;検証内容&lt;/h1&gt;

&lt;p&gt;単語ベクトルは、Github上のREADMEに書いてあるように、東北大学 乾・岡崎研究室で公開されている学習済みのものを利用させて頂いた。&lt;/p&gt;

&lt;p&gt;タスクについては、以下に示すような「鳩」(que) → 「飛ぶ」(target) といった単純な知識を学習させることにする。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;que target
 鳩   飛ぶ
 牛   走る
 鯉   泳ぐ
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;その後、以下のような未学習の単語(que)に対し、学習済みの知識に基づき、適切な単語(target)を想起出来るかどうかを確かめる。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;   que target
 カラス   飛ぶ
    馬   走る
    鯛   泳ぐ
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;期待される結果としては、「カラス」も「鳩」と同じく鳥であることから、「飛ぶ」と連想されること。
原理的には、図のような線上のアトラクタに知識を埋め込むことにより、学習済みのもの(que1)に親しい単語ベクトル(que1&amp;rsquo;)であれば、
引き込まれてtarget1を記憶させた平衡点に至ることで想起が実現される。&lt;/p&gt;

&lt;p&gt;今回は原理確認のため、以下のように事前に単語ベクトル間の近さ（コサイン類似度）を確認した上でタスク設定している。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;  word    カラス      泳ぐ        牛      走る      飛ぶ        馬        鯉        鯛        鳩
カラス  1.000000  0.395338  0.475648  0.162681  0.493993  0.230542  0.406434  0.324465  0.670461
  泳ぐ  0.395338  1.000000  0.166645  0.576415  0.681419  0.162183  0.277283  0.179173  0.385671
    牛  0.475648  0.166645  1.000000  0.083026  0.149580  0.589086  0.559115  0.564100  0.565433
  走る  0.162681  0.576415  0.083026  1.000000  0.508805  0.123983 -0.021091 -0.010774  0.193441
  飛ぶ  0.493993  0.681419  0.149580  0.508805  1.000000  0.110546  0.133639  0.123491  0.422164
    馬  0.230542  0.162183  0.589086  0.123983  0.110546  1.000000  0.251400  0.283276  0.281816
    鯉  0.406434  0.277283  0.559115 -0.021091  0.133639  0.251400  1.000000  0.697661  0.617204
    鯛  0.324465  0.179173  0.564100 -0.010774  0.123491  0.283276  0.697661  1.000000  0.533510
    鳩  0.670461  0.385671  0.565433  0.193441  0.422164  0.281816  0.617204  0.533510  1.000000
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;感覚として近そうな単語を選んだつもりでも、意外と単語ベクトルとしては離れていたりする。
そういう場合は、意図しないアトラクタに引き寄せられてしまうため、注意が必要である。
単語ベクトルは、文章中の使われ方を想定して学習されているため、そういうこともあるかとは思うが、
本来は文脈によって、単語間の距離も適切に遠近するはずである。
それについては、次回以降検証することにしたい。&lt;/p&gt;

&lt;p&gt;さて、以下が実際にテストをしてみた結果である。
queがニューラルネットに初期値として与えられる入力パターン、targetが最終的に到達した出力パターン、throughが想起の途中に接近したパターンである。
例えば、「カラス」を初期値とした場合、学習済みの「鳩」に近づいたあと、その連想記憶先である「飛ぶ」を想起していることが分かる。
他の「馬」「鯛」についても、同様の振る舞いを示しつつ、意図した答えを導き出している。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;open test
que: カラス target: 飛ぶ through: [&#39;カラス&#39;, &#39;鳩&#39;, &#39;飛ぶ&#39;]
que: 馬     target: 走る through: [&#39;馬&#39;, &#39;牛&#39;, &#39;走る&#39;]
que: 鯛     target: 泳ぐ through: [&#39;鯛&#39;, &#39;鯉&#39;, &#39;泳ぐ&#39;]
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;さいごに:92074d8141b5d0b357587f68abb31ec5&#34;&gt;さいごに&lt;/h1&gt;

&lt;p&gt;今回は簡単な事例であったものの、まだあまり手を付けられていないような、全結合型神経回路網を用いた言語処理の可能性を検証した。
全結合型の特徴でもあるエネルギー的な安定状態に記憶を埋め込むことにより、人のように強力な汎化能力をもった記憶を、安定して追加学習できる。
次回以降、実装の詳細や現実的な用途のために必要な、文脈の考慮などについて紹介していきたい。&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>