<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>hassaku&#39;s blog</title>
    <link>http://blog.hassaku-labs.com/</link>
    <description>Recent content on hassaku&#39;s blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>ja</language>
    <lastBuildDate>Sat, 23 Dec 2017 00:59:39 +0900</lastBuildDate>
    <atom:link href="http://blog.hassaku-labs.com/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>partial AUCでprecision重視の評価</title>
      <link>http://blog.hassaku-labs.com/post/partial-auc/</link>
      <pubDate>Sat, 23 Dec 2017 00:59:39 +0900</pubDate>
      
      <guid>http://blog.hassaku-labs.com/post/partial-auc/</guid>
      <description>&lt;p&gt;機械学習の判別器を評価する際、F値やAUCはよく使われていると思います。
しかしながら、実応用の分野によっては、検出出来ないこと(false negative)よりも、誤検出(false positive)の方が問題視されることがよくあります。
そういったprecision重視にしたいケースで使える指標がpartial AUCです。
一般的には、false positiveに制約をもたせて用いるようです。AUCの面積を求める際に、そのfalse positive rateの下限値以上の部分のみ使います。&lt;/p&gt;

&lt;p&gt;簡単に実装するため、scikit learnのaucのコードをオーバーライドして使っています。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;import numpy as np
from sklearn.metrics import roc_auc_score, roc_curve, auc
from sklearn.metrics.base import _average_binary_score

# method overriding
def roc_auc_score(y_true, y_score, average=&amp;quot;macro&amp;quot;, sample_weight=None, max_fpr=None):
    def _binary_roc_auc_score(y_true, y_score, sample_weight=None, max_fpr=max_fpr):
        fpr, tpr, tresholds = roc_curve(y_true, y_score, sample_weight=sample_weight)

        if max_fpr:
            idx = np.where(fpr &amp;lt;= max_fpr)[0]

            idx_last = idx.max()
            idx_next = idx_last + 1
            xc = [fpr[idx_last], fpr[idx_next]]
            yc = [tpr[idx_last], tpr[idx_next]]
            tpr = np.r_[tpr[idx], np.interp(max_fpr, xc, yc)]
            fpr = np.r_[fpr[idx], max_fpr]
            partial_roc = auc(fpr, tpr, reorder=True)

            # standardize result to lie between 0.5 and 1
            min_area = max_fpr**2/2
            max_area = max_fpr
            return 0.5*(1+(partial_roc-min_area)/(max_area-min_area))

        return auc(fpr, tpr, reorder=True)

    return _average_binary_score(_binary_roc_auc_score, y_true, y_score, average, sample_weight=sample_weight)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;以下のような感じで試してみると、AUCが同じスコアであっても、precisionが高い方がpAUCでは比較的高い値になることが分かります。
よって、pAUCを指標に学習やパラメータチューニングを行えば、precision重視のものが出来上がります。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;import matplotlib.pylab as plt
import numpy as np

y_true = np.array([0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0])
y_pred_each_fp = {
    &amp;quot;many&amp;quot;: np.array([0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]),
    &amp;quot;few&amp;quot;: np.array([1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0]),
    &amp;quot;no&amp;quot;: np.array([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0]),
}


MAX_FALSE_POSITIVE_RATE = 0.3

plt.figure(figsize=(17, 5))
for fi, (fp_type, y_pred) in enumerate(y_pred_each_fp.items()):
    plt.subplot(1, 3, fi+1)
    fpr, tpr, _ = roc_curve(y_true, y_pred)
    plt.plot(fpr, tpr, color=&#39;r&#39;, lw=2)
    plt.plot([0, 1], [0, 1], color=&#39;k&#39;, lw=1, linestyle=&#39;--&#39;)
    plt.plot([MAX_FALSE_POSITIVE_RATE, MAX_FALSE_POSITIVE_RATE], [0, 1], color=&#39;k&#39;, lw=1, linestyle=&#39;-.&#39;)
    plt.xlabel(&#39;false positive rate&#39;)
    plt.ylabel(&#39;true positive rate&#39;)
    plt.title(&amp;quot;{fp_type} false positives (auc:{auc:1.2f}  pauc:{pauc:1.2f})&amp;quot;.format(fp_type=fp_type,
        auc=roc_auc_score(y_true, y_pred), pauc=roc_auc_score(y_true, y_pred, max_fpr=MAX_FALSE_POSITIVE_RATE)))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;http://blog.hassaku-labs.com/images/post/pauc/pauc.png&#34; alt=&#34;pauc&#34; /&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>自分流論文メモのやり方</title>
      <link>http://blog.hassaku-labs.com/post/paper-management/</link>
      <pubDate>Fri, 22 Dec 2017 17:17:30 +0900</pubDate>
      
      <guid>http://blog.hassaku-labs.com/post/paper-management/</guid>
      <description>&lt;p&gt;普段から興味ある分野について、arXivを漁っている人多いと思うのですが、やはり手っ取り早く内容を俯瞰するには、abstractが日本語になっている方が負荷が少ないと思います。日本語の概要を一瞬見て、ちゃんと読んだ方がいいかどうか判断する感じです。&lt;/p&gt;

&lt;p&gt;自分は以下のような感じで、arxivのidを貼り付けると、自動でタイトルとか概要を取得して、隣に翻訳も表示してくれるようなスプレッドシートを用意しているのですが、便利なので公開したいと思います。ついでに、実装が公開されていればそのリンクや、簡単な感想なんかもメモするようにしています。&lt;/p&gt;

&lt;iframe src=&#34;https://docs.google.com/spreadsheets/d/e/2PACX-1vRPeZS-GHkCUm6yyhMTXiWfUsbrxa0a_mmNYWygCw1oDuyU7G_v-P2Hhn7iFfT1_HeZFB8NfRJHeIfR/pubhtml?gid=0&amp;amp;single=true&amp;amp;widget=true&amp;amp;headers=false&#34; width=&#34;800&#34; height=&#34;500&#34;&gt;&lt;/iframe&gt;

&lt;p&gt;以下から適当にコピーして頂ければ、使えるかと思います。&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://docs.google.com/spreadsheets/d/19AHvw82bFWivJtoVTohvUDs-uMuhyH0Dw0IzIpvviog&#34;&gt;https://docs.google.com/spreadsheets/d/19AHvw82bFWivJtoVTohvUDs-uMuhyH0Dw0IzIpvviog&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;誰かのお役に立てれば幸いです。。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>hyperoptでパラメータサーチ</title>
      <link>http://blog.hassaku-labs.com/post/hyperopt/</link>
      <pubDate>Sat, 04 Mar 2017 04:08:51 +0900</pubDate>
      
      <guid>http://blog.hassaku-labs.com/post/hyperopt/</guid>
      <description>

&lt;p&gt;機械学習におけるモデルのハイパーパラメータ探索は、その後のモデル評価を正当に行うことにも繋がる、重要な作業です。
近年ではRandom SearchやGrid Search、GA等の各種最適化手法よりも、色々と優れた手法が提案されているらしく、
手軽に使えるようなライブラリも整備されています。
パラメータ探索技術というと、応用範囲が広く、効果が見えやすいため、手軽に使えて効率的な手法があれば、積極的に使っていきたいところです。
その中でもhyperoptというライブラリが、kaggleとかでよく使われているという話を見かけて、試しに使ってみました。&lt;/p&gt;

&lt;p&gt;中身については、色々Blogや論文が見つかるのですが、
Bayesian Optimization -&amp;gt; Sequential Model-based Algorithm Configuration (SMAC) -&amp;gt; Tree-structured Parzen Estimator (TPE)
のように進化してきたTPEという手法が使われているようです。
Bayesian Optimizationのアプローチは、直感的にも効率良さそうなので、その進化系なら期待できます。&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://hyperopt.github.io/hyperopt/&#34;&gt;Hyperopt&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;簡単な使い方:3ef796ded5fdc2a18d77f9e19d0b06e3&#34;&gt;簡単な使い方&lt;/h2&gt;

&lt;p&gt;以下のような感じで、簡単に記述出来て、手軽に取り入れられそうです。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#!/usr/bin/env python
# encoding: utf-8

import os
import sys

import matplotlib.pyplot as plt
import numpy as np
from hyperopt import fmin, tpe, hp, Trials

# パラメータの探索範囲。指定方法は離散値や連続値などに応じて色々ある。
# https://github.com/hyperopt/hyperopt/wiki/FMin#21-parameter-expressions
hyperopt_parameters = { &#39;x&#39;: hp.uniform(&#39;x&#39;, -30, 30) }

# 最小化したい目的関数。正解との誤差とか。
def objective(args):
    return np.sin(args[&#39;x&#39;]) / args[&#39;x&#39;]

# 探索中の進行状況を記録
trials = Trials()

# パラメータサーチの実行
best = fmin(objective,  hyperopt_parameters, algo=tpe.suggest, max_evals=300, trials=trials)

# 目的関数を最小にするパラメータ
print best
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;パラメータサーチの様子を表示:3ef796ded5fdc2a18d77f9e19d0b06e3&#34;&gt;パラメータサーチの様子を表示&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;# 上記コードの続き

x_hisotry = np.ravel([t[&#39;misc&#39;][&#39;vals&#39;][&#39;x&#39;] for t in trials.trials])
objective_history = trials.losses()

fig = plt.figure(figsize=(16, 8))
cm = plt.get_cmap(&#39;jet&#39;)

PLOT_NUM = 5

for i, hist_num in enumerate(np.linspace(50, 300, PLOT_NUM)):
    cmap_cycle = [cm(1. * h/ (hist_num - 1)) for h in range(int(hist_num) - 1)]

    ax = plt.subplot(2, PLOT_NUM, i + 1)
    ax.set_color_cycle(cmap_cycle)
    ax.plot(np.arange(-30, 30, 0.1), function(np.arange(-30, 30, 0.1)), alpha=0.2)
    for j in range(int(hist_num)  - 1):
        ax.plot(x_hisotry[j], objective_history[j], &#39;.&#39;)
    ax.set_title(&#39;times: {times}&#39;.format(times=int(hist_num)))
    ax.set_ylim([np.min(objective_history) - 0.1, np.max(objective_history) + 0.1])
    ax.set_xlim([-30, 30])
    if i == 0:
        ax.set_ylabel(&#39;y&#39;)

    plt.subplot(2, PLOT_NUM, PLOT_NUM + i + 1)
    plt.hist(x_hisotry[:hist_num], bins=50)
    if i == 0:
        plt.ylabel(&#39;histogram of x&#39;)
    plt.xlabel(&#39;x&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;http://blog.hassaku-labs.com/images/post/hyperopt/hyperopt.png&#34; alt=&#34;hyperopt&#34; /&gt;&lt;/p&gt;

&lt;p&gt;右に進むに連れて、パラメータ探索が進んでいく様子を表しています。
上段の図が、目的関数と探索点の描画です。x=0の点が求めたいパラメータとなります。
下段の図が、各探索点のヒストグラムを描画しているのですが、探索が進むにつれて、
目標のパラメータ付近を効率的に探索している様子が分かります。&lt;/p&gt;

&lt;h4 id=&#34;追記:3ef796ded5fdc2a18d77f9e19d0b06e3&#34;&gt;追記&lt;/h4&gt;

&lt;p&gt;バンディットベースのものが出たらしい。使い方もhyperopt同様に簡単そうなので、パラメータチューニング時の候補にしたい。&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/zygmuntz/hyperband&#34;&gt;Hyperband&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>インスタンスを表す文字列を分かりやすくする</title>
      <link>http://blog.hassaku-labs.com/post/class-repr/</link>
      <pubDate>Sat, 04 Mar 2017 04:07:38 +0900</pubDate>
      
      <guid>http://blog.hassaku-labs.com/post/class-repr/</guid>
      <description>

&lt;p&gt;ログとかにインスタンスの内容をダンプしたりするとき、つい適当にprintとかlogger.debugしても、
標準では、インスタンスの内容をスマートに分かりやすく表示してくれたりはしないようです。&lt;/p&gt;

&lt;p&gt;そのため、デバッグをしやすくするためにも、以下のような感じで__repr__をオーバーライドしておくと、
分かりやすくなって便利だったりします。&lt;/p&gt;

&lt;h2 id=&#34;reprを定義しない場合:9c4edd90300eaacc62e25ae2f1faec5e&#34;&gt;reprを定義しない場合&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;class MyClass(object):
  def __init__(self):
    self.attr1 = 123
    self.attr2 = 456

mc = MyClass()
print(mc)  # =&amp;gt; &amp;lt;__main__.MyClass object at 0xXXXXXXXXXX&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;reprを定義する場合:9c4edd90300eaacc62e25ae2f1faec5e&#34;&gt;reprを定義する場合&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;class MyClass(object):
  def __init__(self):
    self.attr1 = 123
    self.attr2 = 456

  def __repr__(self):
    return &amp;quot;, &amp;quot;.join(&amp;quot;%s: %s&amp;quot; % item for item in vars(self).items())

mc = MyClass()
print(mc)  # =&amp;gt; attr2: 456, attr1: 123
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ちょっとしたTipsでした。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>ブラウザで動作出来なくなったJavaアプレットをローカルで動かす</title>
      <link>http://blog.hassaku-labs.com/post/run-applet/</link>
      <pubDate>Sun, 26 Feb 2017 03:56:14 +0900</pubDate>
      
      <guid>http://blog.hassaku-labs.com/post/run-applet/</guid>
      <description>&lt;p&gt;昔作って公開していたJAVAアプレットを発掘したものの、
ブラウザやJavaのセキュリティ設定をいじっても安定して表示することが出来ませんでした。
例外サイトとかに追加しても、ネットワーク切り替えなどちょっとしたきっかけで実行不能に&amp;hellip;&lt;/p&gt;

&lt;p&gt;いろいろ調べた結果、Macの場合、以下のような感じでダウンロードしてから実行することで、
安定して動作させることが出来ました。
公開は諦めて、とりあえず身近な人に見せるだけなら、これで良さそうです。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ wget --no-parent -r &amp;quot;javaアプレットのファイル一式が入ったディレクトリXXXXのURL&amp;quot;
$ cd XXXX
$ appletviewer XXXX/main.html
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Javaアプレット、一時期はまってたなぁ&amp;hellip;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>学習済みword2vecモデルを調べてみた</title>
      <link>http://blog.hassaku-labs.com/post/pretrained-word2vec/</link>
      <pubDate>Sun, 26 Feb 2017 02:27:30 +0900</pubDate>
      
      <guid>http://blog.hassaku-labs.com/post/pretrained-word2vec/</guid>
      <description>

&lt;p&gt;日本語の自然言語処理で分散表現を使おうと思った場合、まず頭に浮かぶのはword2vecだと思います。
特に分散表現自体の精度とかには興味がなく、それを使った対話システムを作りたいだけだったりするのであれば、
データクレンジングや学習には結構時間もかかるので、学習済みの公開モデルを使わせていただくのが手っ取り早そうです。&lt;/p&gt;

&lt;p&gt;(単語ベクトルの準備に手間取り、モチベーション低下に繋がる悪い例：&lt;a href=&#34;http://blog.hassaku-labs.com/post/dialogue_system1/&#34;&gt;対話システムを作りたい！【準備編１】&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;調べてみると、よく出来ていそうな公開モデルを２つ見つけたので、その利用方法と気になるベクトル次元数と単語数を調べてみました。
なお、どちらもWikipedia日本語版を学習元にしているようです。&lt;/p&gt;

&lt;p&gt;word2vecを使うには、以下のバージョンのgensimを利用します。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ pip freeze | grep gensim
gensim==1.0.0
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;白ヤギコーポレーションのモデル:e37dac6f2b05ab116e13b711e87f0489&#34;&gt;白ヤギコーポレーションのモデル&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;http://aial.shiroyagi.co.jp/2017/02/japanese-word2vec-model-builder/&#34;&gt;word2vecの学習済み日本語モデルを公開します&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; from gensim.models import word2vec
&amp;gt;&amp;gt;&amp;gt; model = word2vec.Word2Vec.load(&#39;./shiroyagi/word2vec.gensim.model&#39;)
&amp;gt;&amp;gt;&amp;gt; model[u&#39;ニュース&#39;].shape
(50,)
&amp;gt;&amp;gt;&amp;gt; model.corpus_count
1046708
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;東北大学-乾-岡崎研究室のモデル:e37dac6f2b05ab116e13b711e87f0489&#34;&gt;東北大学 乾・岡崎研究室のモデル&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;http://www.cl.ecei.tohoku.ac.jp/~m-suzuki/jawiki_vector/&#34;&gt;日本語 Wikipedia エンティティベクトル&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; from gensim.models import KeyedVectors
&amp;gt;&amp;gt;&amp;gt; model = KeyedVectors.load_word2vec_format(&#39;./tohoku_entity_vector/entity_vector.model.bin&#39;, binary=True)
&amp;gt;&amp;gt;&amp;gt; model[u&#39;ニュース&#39;].shape
(200,)
&amp;gt;&amp;gt;&amp;gt; len(model.vocab)
1005367
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;gensimが1-0-0未満の場合-ただしシロヤギさんのモデルは1-0-0以上が必要のようです:e37dac6f2b05ab116e13b711e87f0489&#34;&gt;gensimが1.0.0未満の場合（ただしシロヤギさんのモデルは1.0.0以上が必要のようです）&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; from gensim.models import word2vec
&amp;gt;&amp;gt;&amp;gt; model = word2vec.Word2Vec.load_word2vec_format(&#39;./tohoku_entity_vector/entity_vector.model.bin&#39;, binary=True)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;追記-facebookの学習済みfasttextモデル:e37dac6f2b05ab116e13b711e87f0489&#34;&gt;（追記）Facebookの学習済みFastTextモデル&lt;/h2&gt;

&lt;p&gt;後日、FacebookのFastTextのレポジトリでも、日本語Wikipediaの分散表現モデルが公開されているのを見つけました。
ボキャブラリ数は少ないみたいですが、ベクトルの次元数が一番大きいです。
もしかすると、ボキャブラリ数に関しては、容量の観点から、不要な用語を除去するなど、うまくデータクレンジングされた結果なのかもしれません。
利用時には要調査です。&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/facebookresearch/fastText/blob/master/pretrained-vectors.md&#34;&gt;Pre-trained word vectors&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; from gensim.models import KeyedVectors
&amp;gt;&amp;gt;&amp;gt; model = KeyedVectors.load_word2vec_format(&#39;./wiki.ja.vec&#39;)  # binは読み込めないらしい
&amp;gt;&amp;gt;&amp;gt; model[u&#39;ニュース&#39;].shape
(300,)
&amp;gt;&amp;gt;&amp;gt; len(model.vocab)
580000
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;まとめ:e37dac6f2b05ab116e13b711e87f0489&#34;&gt;まとめ&lt;/h2&gt;

&lt;p&gt;同じ日本語Wikipediaを学習しているので、ボキャブラリの数は大体同じく約100万程度と十分そうな感じです。
ベクトルの次元数については、白ヤギさんのモデルの方が小さく、動作は軽そうです。
一方で、東北大学のモデルの方が、単語の表現力は高そうなので、色々検証レベルでは色々と無難な気がします。
実はもう少し高次元での処理をしたいので、更にこれをスパース（出来れば２値ベクトル）に変換するようなこと
（こういうやつ &lt;a href=&#34;http://www.manaalfaruqui.com/papers/acl15-overcomplete.pdf&#34;&gt;Sparse Overcomplete Word Vector Representations&lt;/a&gt; ）
をやりたいのですが、それはまた次回にでも。。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>対話システムを作りたい！【準備編１】</title>
      <link>http://blog.hassaku-labs.com/post/dialogue_system1/</link>
      <pubDate>Fri, 01 Apr 2016 02:26:21 +0900</pubDate>
      
      <guid>http://blog.hassaku-labs.com/post/dialogue_system1/</guid>
      <description>

&lt;p&gt;2016年はVRとか流行りそうで、仮想空間での生活を妄想してしまう今日このごろ。
でも、今のままだとNPCがちゃんと自然に会話してくれない気がして微妙なんですよね。。。そこで、既存技術の延長で、どれくらいの日本語対話が可能か、ちょっと自分でも作ってみたくなりました。&lt;/p&gt;

&lt;p&gt;自然言語処理をちゃんと勉強したことはないけれど、脳型情報処理アプローチでいくとしたら、結局はベクトル時系列データの処理なのかな？って思います。とりあえず、色々試してみましょう。&lt;/p&gt;

&lt;p&gt;たぶん、進め方はこんな感じ。&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;単語のベクトルデータ化（言語コーパス？）&lt;/li&gt;
&lt;li&gt;対話データのベクトル時系列データ化（対話コーパス？）&lt;/li&gt;
&lt;li&gt;会話時系列データにおける応答時系列データの予測学習&lt;/li&gt;
&lt;li&gt;学習結果を用いた対話システム構築&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;というわけで、今回は１の言語コーパス作成について。単語を入力とし、N次元ベクトルに変換することを目標。似たような単語は近くに配置されるような変換が好ましい（分散表現だ！）。&lt;/p&gt;

&lt;h1 id=&#34;言語コーパスをwikipediaの記事から作成:9f533d3f859ab3c9a17cb0f76df4e5f1&#34;&gt;言語コーパスをWikipediaの記事から作成&lt;/h1&gt;

&lt;h2 id=&#34;wikipediaの日本語記事をダウンロード:9f533d3f859ab3c9a17cb0f76df4e5f1&#34;&gt;wikipediaの日本語記事をダウンロード&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;$ curl -O http://dumps.wikimedia.org/jawiki/latest/jawiki-latest-pages-articles.xml.bz
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;mecabをインストール:9f533d3f859ab3c9a17cb0f76df4e5f1&#34;&gt;mecabをインストール&lt;/h2&gt;

&lt;p&gt;単語の分かち書きへ変換するためのツールです。macならbrewでインストール可能。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ brew install mecab
$ brew install mecab-ipadic
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;新語用辞書をインストール:9f533d3f859ab3c9a17cb0f76df4e5f1&#34;&gt;新語用辞書をインストール&lt;/h3&gt;

&lt;p&gt;最近の単語は、brewでインストールされた辞書には含まれていないので、新語に対応した辞書に更新します。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ git clone --depth 1 git@github.com:neologd/mecab-ipadic-neologd.git
$ cd mecab-ipadic-neologd/
$ ./bin/install-mecab-ipadic-neologd -n   # 辞書updateも同じコマンド
$ echo `mecab-config --dicdir`&amp;quot;/mecab-ipadic-neologd&amp;quot;  # 実行時指定のパスを調べる
/usr/local/lib/mecab/dic/mecab-ipadic-neologd
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;参考-新御用辞書有無を確認:9f533d3f859ab3c9a17cb0f76df4e5f1&#34;&gt;（参考）新御用辞書有無を確認&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;$ pip install mecab
$ python
In [1]: import MeCab
In [2]: mecab_org = MeCab.Tagger(&amp;quot;-Owakati&amp;quot;)
In [3]: mecab_new = MeCab.Tagger(&amp;quot;-Owakati -d /usr/local/lib/mecab/dic/mecab-ipadic-neologd&amp;quot;)
In [4]: print mecab_org.parse(&amp;quot;電力自由化がはじまる&amp;quot;)
電力 自由 化 が はじまる
In [5]: print mecab_new.parse(&amp;quot;電力自由化がはじまる&amp;quot;)
電力自由化 が はじまる
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;なんとなく最近ニュースとかで出てくるような単語を分かち書き出来ている（気がします）。&lt;/p&gt;

&lt;h2 id=&#34;wikipediaを分かち書き:9f533d3f859ab3c9a17cb0f76df4e5f1&#34;&gt;wikipediaを分かち書き&lt;/h2&gt;

&lt;p&gt;各単語に分かち書き変換します。&lt;/p&gt;

&lt;h3 id=&#34;htmlタグとかを取っ払って文章だけに変換:9f533d3f859ab3c9a17cb0f76df4e5f1&#34;&gt;HTMLタグとかを取っ払って文章だけに変換&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;$ echo &#39;gem &amp;quot;wp2txt&amp;quot;&#39; &amp;gt;&amp;gt; Gemfile
$ bundle
$ bundle exec wp2txt --input-file jawiki-latest-pages-articles.xml.bz
$ ls wp2txt/  # 変換結果
$ rm jawiki-latest-pages-articles.xml.bz # 不要だから消す
$ cat wp2txt/jawiki-latest-pages-articles-* &amp;gt; corpus.txt # 変換結果を一つのファイルに連結
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;とても時間かかります&amp;hellip;
記号除去とかした方が良いのかどうか、今のところ謎です。&lt;/p&gt;

&lt;h3 id=&#34;新語辞書使って分かち書き:9f533d3f859ab3c9a17cb0f76df4e5f1&#34;&gt;新語辞書使って分かち書き&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;$ mecab -Owakati -d /usr/local/lib/mecab/dic/mecab-ipadic-neologd corpus.txt &amp;gt; corpus_wakati.txt
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;とても時間かかります&amp;hellip;&lt;/p&gt;

&lt;h2 id=&#34;word2vecによるベクトル化:9f533d3f859ab3c9a17cb0f76df4e5f1&#34;&gt;word2vecによるベクトル化&lt;/h2&gt;

&lt;p&gt;今後pythonで実装することもあり、gensim使います。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ pip install gensim
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;utf-8に変換:9f533d3f859ab3c9a17cb0f76df4e5f1&#34;&gt;utf-8に変換&lt;/h3&gt;

&lt;p&gt;しないとword2vecのときに、文字コードについて怒られたので&amp;hellip;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ iconv -c -t UTF-8 &amp;lt; corpus_wakati.txt &amp;gt; corpus_wakati_utf-8.txt
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;学習:9f533d3f859ab3c9a17cb0f76df4e5f1&#34;&gt;学習&lt;/h3&gt;

&lt;p&gt;以下のpythonコードを実行します。次元は適当に中程度としました。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ vi word2vec_train.py
# coding: utf-8
from gensim.models import word2vec
import sys, logging, string, codecs

logging.basicConfig(format=&#39;%(asctime)s : %(levelname)s : %(message)s&#39;, level=logging.INFO)

# 学習（400次元だと４コアで約３時間...）
sentences = word2vec.Text8Corpus(&amp;quot;corpus_wakati_utf-8.txt&amp;quot;)
model = word2vec.Word2Vec(sentences, size=400, workers=4)
# モデルの保存
model.save(&amp;quot;w2v_model_%d_dims&amp;quot; % dims)
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;検証:9f533d3f859ab3c9a17cb0f76df4e5f1&#34;&gt;検証&lt;/h3&gt;

&lt;h4 id=&#34;モデルの読み込み:9f533d3f859ab3c9a17cb0f76df4e5f1&#34;&gt;モデルの読み込み&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;$ pyton
In [1]: from gensim.models import word2vec
In [2]: model = word2vec.Word2Vec.load(&amp;quot;./word2vec_models/w2v_model_%d_dims&amp;quot; % 400)
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;ベクトル空間上で近い単語を探す:9f533d3f859ab3c9a17cb0f76df4e5f1&#34;&gt;ベクトル空間上で近い単語を探す&lt;/h4&gt;

&lt;p&gt;動作確認です。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;In [3]: most_similar = model.most_similar(positive=[u&#39;サッカー&#39;])[0]
In [4]: most_similar[0]
ラグビー
In [5]: most_similar[1]
0.663492918015  # コサイン距離？
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;ベクトル取得:9f533d3f859ab3c9a17cb0f76df4e5f1&#34;&gt;ベクトル取得&lt;/h4&gt;

&lt;p&gt;このベクトルに対して今後処理していくことになります。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;In [1]: vector = model[u&#39;サッカー&#39;]
array([  2.69545317e-01,  -1.99663490e-01,   9.52050760e-02,
         2.16732353e-01,   1.97090670e-01,  -1.90409079e-01,
         ...
In [2]: vector.shape
(400,)
In [3]: vector.min()
-0.71880466
In [4]: vector.max()
0.75658286
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;ベクトルから単語を探す:9f533d3f859ab3c9a17cb0f76df4e5f1&#34;&gt;ベクトルから単語を探す&lt;/h4&gt;

&lt;p&gt;処理結果のベクトルから単語を復元する手段も確認しておきます。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;In [5]: vector[0:10] = 0.0  # 適当に変更
In [6]: for cname in [candidate[0] for candidate in model.most_similar(positive=[vector], topn=3)]:
            print cname
サッカー
ラグビー
フットサル
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;おわり:9f533d3f859ab3c9a17cb0f76df4e5f1&#34;&gt;おわり&lt;/h2&gt;

&lt;p&gt;ひとまず今回はここまで。単語をベクトル化したことにより、文章をベクトル時系列データとして扱うことが出来、色々な機械学習手法が適用可能になりました。
次回以降色々試してみたいと思います。&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&#34;おまけ:9f533d3f859ab3c9a17cb0f76df4e5f1&#34;&gt;おまけ&lt;/h1&gt;

&lt;p&gt;この時点でも色々作って遊べますね。例えば、以前作った絵文字サジェスト用hubotでは、キーワードに近しい、絵文字キーワードを引っ張ってきて候補を表示してました。Githubとかだと、標準でも絵文字キーワードを入力すると、続きをサジェストしてくれるものの、そもそものキーワードが思いつかなかったりするので、そういうときに便利です :)&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://blog.hassaku-labs.com/images/post/dialogue_system1/word2emoji.jpg&#34; alt=&#34;word2emoji.jpg&#34; /&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>exception-slackerというライブラリをPyPIに登録した</title>
      <link>http://blog.hassaku-labs.com/post/exception_slacker/</link>
      <pubDate>Fri, 01 Apr 2016 00:35:40 +0900</pubDate>
      
      <guid>http://blog.hassaku-labs.com/post/exception_slacker/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://github.com/hassaku/exception-slacker&#34;&gt;https://github.com/hassaku/exception-slacker&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;importして、Slackのtokenや投稿チャンネルなどを環境変数でセットしておけば、例外が発生した際に、以下のような感じでSlackへ投稿します。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://blog.hassaku-labs.com/images/post/exception_slacker/exception_slacker.png&#34; alt=&#34;exception_slacker&#34; /&gt;&lt;/p&gt;

&lt;p&gt;時間のかかるプロセスなどを実行している時に、いちいちコンソールをチェックする手間が省けるので、個人的に便利だと思って公開しました。&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>