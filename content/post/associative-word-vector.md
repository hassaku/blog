+++
date = "2018-07-15T00:00:00+09:00"
description = ""
tags = ["研究"]
draft = false
title = "単語ベクトルと全結合ニューラルネットワークによる単語連想記憶"
+++

自然言語処理にニューラルネットワークを適用する事例が増えている。
ここでは、従来の部分的に再帰結合をもつようなRNN(LSTM)といった構造ではなく、
(最近はあまり流行っていない）ホップフィールドモデルのような全結合構造のニューラルネットを用いて、
エネルギーポテンシャルの窪み、引き込み領域を有するアトラクタ空間に言語知識を記憶させることを目指す。

![potential](/images/post/associative-word-vector/potential.png)

メリットとして、以下のようなことが挙げられる。

- 追加学習が容易
- データを大量に学習しなくても汎化性能が高い

いくつか想定している内容のうち、今回は単語ベクトルのペアを連想記憶する基本的なタスクを検証する。

# 検証用コード

https://github.com/hassaku/associative-word-vector/tree/blog-1

# 検証内容

単語ベクトルは、Github上のREADMEに書いてあるように、東北大学 乾・岡崎研究室で公開されている学習済みのものを利用させて頂いた。

タスクについては、以下に示すような「鳩」(que) → 「飛ぶ」(target) といった単純な知識を学習させることにする。

```
que target
 鳩   飛ぶ
 牛   走る
 鯉   泳ぐ
```

その後、以下のような未学習の単語(que)に対し、学習済みの知識に基づき、適切な単語(target)を想起出来るかどうかを確かめる。

```
   que target
 カラス   飛ぶ
    馬   走る
    鯛   泳ぐ
```

期待される結果としては、「カラス」も「鳩」と同じく鳥であることから、「飛ぶ」と連想されること。
原理的には、図のような線上のアトラクタに知識を埋め込むことにより、学習済みのもの(que1)に親しい単語ベクトル(que1')であれば、
引き込まれてtarget1を記憶させた平衡点に至ることで想起が実現される。

今回は原理確認のため、以下のように事前に単語ベクトル間の近さ（コサイン類似度）を確認した上でタスク設定している。

```
  word    カラス      泳ぐ        牛      走る      飛ぶ        馬        鯉        鯛        鳩  
カラス  1.000000  0.395338  0.475648  0.162681  0.493993  0.230542  0.406434  0.324465  0.670461
  泳ぐ  0.395338  1.000000  0.166645  0.576415  0.681419  0.162183  0.277283  0.179173  0.385671
    牛  0.475648  0.166645  1.000000  0.083026  0.149580  0.589086  0.559115  0.564100  0.565433
  走る  0.162681  0.576415  0.083026  1.000000  0.508805  0.123983 -0.021091 -0.010774  0.193441
  飛ぶ  0.493993  0.681419  0.149580  0.508805  1.000000  0.110546  0.133639  0.123491  0.422164
    馬  0.230542  0.162183  0.589086  0.123983  0.110546  1.000000  0.251400  0.283276  0.281816
    鯉  0.406434  0.277283  0.559115 -0.021091  0.133639  0.251400  1.000000  0.697661  0.617204
    鯛  0.324465  0.179173  0.564100 -0.010774  0.123491  0.283276  0.697661  1.000000  0.533510
    鳩  0.670461  0.385671  0.565433  0.193441  0.422164  0.281816  0.617204  0.533510  1.000000
```

感覚として近そうな単語を選んだつもりでも、意外と単語ベクトルとしては離れていたりする。
そういう場合は、意図しないアトラクタに引き寄せられてしまうため、注意が必要である。
単語ベクトルは、文章中の使われ方を想定して学習されているため、そういうこともあるかとは思うが、
本来は文脈によって、単語間の距離も適切に遠近するはずである。
それについては、次回以降検証することにしたい。

さて、以下が実際にテストをしてみた結果である。
queがニューラルネットに初期値として与えられる入力パターン、targetが最終的に到達した出力パターン、throughが想起の途中に接近したパターンである。
例えば、「カラス」を初期値とした場合、学習済みの「鳩」に近づいたあと、その連想記憶先である「飛ぶ」を想起していることが分かる。
他の「馬」「鯛」についても、同様の振る舞いを示しつつ、意図した答えを導き出している。

```
open test
que: カラス target: 飛ぶ through: ['カラス', '鳩', '飛ぶ']
que: 馬     target: 走る through: ['馬', '牛', '走る']
que: 鯛     target: 泳ぐ through: ['鯛', '鯉', '泳ぐ']
```

# さいごに

今回は簡単な事例であったものの、まだあまり手を付けられていないような、全結合型神経回路網を用いた言語処理の可能性を検証した。
全結合型の特徴でもあるエネルギー的な安定状態に記憶を埋め込むことにより、人のように強力な汎化能力をもった記憶を、安定して追加学習できる。
次回以降、実装の詳細や現実的な用途のために必要な、文脈の考慮などについて紹介していきたい。


