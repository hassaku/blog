<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Nlp on hassaku&#39;s blog</title>
    <link>http://blog.hassaku-labs.com/tags/nlp/index.xml</link>
    <description>Recent content in Nlp on hassaku&#39;s blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>ja</language>
    <copyright>Written by hassaku</copyright>
    <atom:link href="http://blog.hassaku-labs.com/tags/nlp/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>word vectorのような読み込みが重たいやつをWebAPI化して軽量化</title>
      <link>http://blog.hassaku-labs.com/post/word-vector-api/</link>
      <pubDate>Mon, 25 Feb 2019 10:00:00 +0900</pubDate>
      
      <guid>http://blog.hassaku-labs.com/post/word-vector-api/</guid>
      <description>&lt;p&gt;word vectorとかメモリをどデカく使うようなやつは、毎回スクリプトを起動する際に読み込みに時間がかかって辛い。
そういうのは、極力別のプロセスにして、適当にAPIとか生やして連携するようにしておくと楽チンなので良くやるパターン。&lt;/p&gt;

&lt;p&gt;以下は、単語ベクトルを返してくれるAPIを作った例。Flask使うとコードもシンプルに実現出来るので良い。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# coding: utf-8

import numpy as np
import gensim
from flask import Flask, jsonify, request
import json

PRETRAINED_W2V_PATH = &#39;./model.bin&#39;

app = Flask(__name__)
app.config[&#39;JSON_AS_ASCII&#39;] = False

model = gensim.models.KeyedVectors.load_word2vec_format(PRETRAINED_W2V_PATH, binary=True)  # 超時間かかる処理

@app.route(&#39;/word_vector&#39;, methods=[&#39;GET&#39;])
def word_vector():
    word = request.args.get(&#39;word&#39;)
    vector = np.array(model[word]).astype(float).tolist()
    return jsonify({&#39;vector&#39;: vector}), 200


if __name__ == &amp;quot;__main__&amp;quot;:
    app.debug = True
    app.run(host=&#39;0.0.0.0&#39;, port=8888)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;以下のような感じで単語ベクトルの値をjsonで返してくれる。pythonのスクリプトからはrequestsとかで簡単に取得して扱えるはず。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ curl &amp;quot;http://0.0.0.0:8888/word_vector?word=テスト&amp;quot;
{
  &amp;quot;vector&amp;quot;: [
    0.029713749885559082,
    -0.6024296283721924,
    0.9723357558250427,
    -1.1497808694839478,
    1.3764394521713257,
...

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;例えば、連続動作しているようなエージェントシミュレータなんかにも、似たような感じでAPI生やして、インタラクションさせることが出来る。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>学習済みword2vecモデルを調べてみた</title>
      <link>http://blog.hassaku-labs.com/post/pretrained-word2vec/</link>
      <pubDate>Sun, 26 Feb 2017 02:27:30 +0900</pubDate>
      
      <guid>http://blog.hassaku-labs.com/post/pretrained-word2vec/</guid>
      <description>

&lt;p&gt;日本語の自然言語処理で分散表現を使おうと思った場合、まず頭に浮かぶのはword2vecだと思います。
特に分散表現自体の精度とかには興味がなく、それを使った対話システムを作りたいだけだったりするのであれば、
データクレンジングや学習には結構時間もかかるので、学習済みの公開モデルを使わせていただくのが手っ取り早そうです。&lt;/p&gt;

&lt;p&gt;(単語ベクトルの準備に手間取り、モチベーション低下に繋がる悪い例：&lt;a href=&#34;http://blog.hassaku-labs.com/post/dialogue_system1/&#34;&gt;対話システムを作りたい！【準備編１】&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;調べてみると、よく出来ていそうな公開モデルを２つ見つけたので、その利用方法と気になるベクトル次元数と単語数を調べてみました。
なお、どちらもWikipedia日本語版を学習元にしているようです。&lt;/p&gt;

&lt;p&gt;word2vecを使うには、以下のバージョンのgensimを利用します。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ pip freeze | grep gensim
gensim==1.0.0
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;白ヤギコーポレーションのモデル&#34;&gt;白ヤギコーポレーションのモデル&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;http://aial.shiroyagi.co.jp/2017/02/japanese-word2vec-model-builder/&#34;&gt;word2vecの学習済み日本語モデルを公開します&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; from gensim.models import word2vec
&amp;gt;&amp;gt;&amp;gt; model = word2vec.Word2Vec.load(&#39;./shiroyagi/word2vec.gensim.model&#39;)
&amp;gt;&amp;gt;&amp;gt; model[u&#39;ニュース&#39;].shape
(50,)
&amp;gt;&amp;gt;&amp;gt; model.corpus_count
1046708
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;東北大学-乾-岡崎研究室のモデル&#34;&gt;東北大学 乾・岡崎研究室のモデル&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;http://www.cl.ecei.tohoku.ac.jp/~m-suzuki/jawiki_vector/&#34;&gt;日本語 Wikipedia エンティティベクトル&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; from gensim.models import KeyedVectors
&amp;gt;&amp;gt;&amp;gt; model = KeyedVectors.load_word2vec_format(&#39;./tohoku_entity_vector/entity_vector.model.bin&#39;, binary=True)
&amp;gt;&amp;gt;&amp;gt; model[u&#39;ニュース&#39;].shape
(200,)
&amp;gt;&amp;gt;&amp;gt; len(model.vocab)
1005367
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;gensimが1-0-0未満の場合-ただしシロヤギさんのモデルは1-0-0以上が必要のようです&#34;&gt;gensimが1.0.0未満の場合（ただしシロヤギさんのモデルは1.0.0以上が必要のようです）&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; from gensim.models import word2vec
&amp;gt;&amp;gt;&amp;gt; model = word2vec.Word2Vec.load_word2vec_format(&#39;./tohoku_entity_vector/entity_vector.model.bin&#39;, binary=True)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;追記-facebookの学習済みfasttextモデル&#34;&gt;（追記）Facebookの学習済みFastTextモデル&lt;/h2&gt;

&lt;p&gt;後日、FacebookのFastTextのレポジトリでも、日本語Wikipediaの分散表現モデルが公開されているのを見つけました。
ボキャブラリ数は少ないみたいですが、ベクトルの次元数が一番大きいです。
もしかすると、ボキャブラリ数に関しては、容量の観点から、不要な用語を除去するなど、うまくデータクレンジングされた結果なのかもしれません。
利用時には要調査です。&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/facebookresearch/fastText/blob/master/pretrained-vectors.md&#34;&gt;Pre-trained word vectors&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; from gensim.models import KeyedVectors
&amp;gt;&amp;gt;&amp;gt; model = KeyedVectors.load_word2vec_format(&#39;./wiki.ja.vec&#39;)  # binは読み込めないらしい
&amp;gt;&amp;gt;&amp;gt; model[u&#39;ニュース&#39;].shape
(300,)
&amp;gt;&amp;gt;&amp;gt; len(model.vocab)
580000
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;まとめ&#34;&gt;まとめ&lt;/h2&gt;

&lt;p&gt;同じ日本語Wikipediaを学習しているので、ボキャブラリの数は大体同じく約100万程度と十分そうな感じです。
ベクトルの次元数については、白ヤギさんのモデルの方が小さく、動作は軽そうです。
一方で、東北大学のモデルの方が、単語の表現力は高そうなので、色々検証レベルでは色々と無難な気がします。
実はもう少し高次元での処理をしたいので、更にこれをスパース（出来れば２値ベクトル）に変換するようなこと
（こういうやつ &lt;a href=&#34;http://www.manaalfaruqui.com/papers/acl15-overcomplete.pdf&#34;&gt;Sparse Overcomplete Word Vector Representations&lt;/a&gt; ）
をやりたいのですが、それはまた次回にでも。。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>対話システムを作りたい！（実際はWikipediaのデータを取得して単語ベクトルを学習するまで）</title>
      <link>http://blog.hassaku-labs.com/post/dialogue_system1/</link>
      <pubDate>Fri, 01 Apr 2016 02:26:21 +0900</pubDate>
      
      <guid>http://blog.hassaku-labs.com/post/dialogue_system1/</guid>
      <description>

&lt;p&gt;2016年はVRとか流行りそうで、仮想空間での生活を妄想してしまう今日このごろ。
でも、今のままだとNPCがちゃんと自然に会話してくれない気がして微妙なんですよね。。。そこで、既存技術の延長で、どれくらいの日本語対話が可能か、ちょっと自分でも作ってみたくなりました。&lt;/p&gt;

&lt;p&gt;自然言語処理をちゃんと勉強したことはないけれど、脳型情報処理アプローチでいくとしたら、結局はベクトル時系列データの処理なのかな？って思います。とりあえず、色々試してみましょう。&lt;/p&gt;

&lt;p&gt;たぶん、進め方はこんな感じ。&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;単語のベクトルデータ化（言語コーパス？）&lt;/li&gt;
&lt;li&gt;対話データのベクトル時系列データ化（対話コーパス？）&lt;/li&gt;
&lt;li&gt;会話時系列データにおける応答時系列データの予測学習&lt;/li&gt;
&lt;li&gt;学習結果を用いた対話システム構築&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;というわけで、今回は１の言語コーパス作成について。単語を入力とし、N次元ベクトルに変換することを目標。似たような単語は近くに配置されるような変換が好ましい（分散表現だ！）。&lt;/p&gt;

&lt;h1 id=&#34;言語コーパスをwikipediaの記事から作成&#34;&gt;言語コーパスをWikipediaの記事から作成&lt;/h1&gt;

&lt;h2 id=&#34;wikipediaの日本語記事をダウンロード&#34;&gt;wikipediaの日本語記事をダウンロード&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;$ curl -O http://dumps.wikimedia.org/jawiki/latest/jawiki-latest-pages-articles.xml.bz
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;mecabをインストール&#34;&gt;mecabをインストール&lt;/h2&gt;

&lt;p&gt;単語の分かち書きへ変換するためのツールです。macならbrewでインストール可能。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ brew install mecab
$ brew install mecab-ipadic
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;新語用辞書をインストール&#34;&gt;新語用辞書をインストール&lt;/h3&gt;

&lt;p&gt;最近の単語は、brewでインストールされた辞書には含まれていないので、新語に対応した辞書に更新します。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ git clone --depth 1 git@github.com:neologd/mecab-ipadic-neologd.git
$ cd mecab-ipadic-neologd/
$ ./bin/install-mecab-ipadic-neologd -n   # 辞書updateも同じコマンド
$ echo `mecab-config --dicdir`&amp;quot;/mecab-ipadic-neologd&amp;quot;  # 実行時指定のパスを調べる
/usr/local/lib/mecab/dic/mecab-ipadic-neologd
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;参考-新御用辞書有無を確認&#34;&gt;（参考）新御用辞書有無を確認&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;$ pip install mecab
$ python
In [1]: import MeCab
In [2]: mecab_org = MeCab.Tagger(&amp;quot;-Owakati&amp;quot;)
In [3]: mecab_new = MeCab.Tagger(&amp;quot;-Owakati -d /usr/local/lib/mecab/dic/mecab-ipadic-neologd&amp;quot;)
In [4]: print mecab_org.parse(&amp;quot;電力自由化がはじまる&amp;quot;)
電力 自由 化 が はじまる
In [5]: print mecab_new.parse(&amp;quot;電力自由化がはじまる&amp;quot;)
電力自由化 が はじまる
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;なんとなく最近ニュースとかで出てくるような単語を分かち書き出来ている（気がします）。&lt;/p&gt;

&lt;h2 id=&#34;wikipediaを分かち書き&#34;&gt;wikipediaを分かち書き&lt;/h2&gt;

&lt;p&gt;各単語に分かち書き変換します。&lt;/p&gt;

&lt;h3 id=&#34;htmlタグとかを取っ払って文章だけに変換&#34;&gt;HTMLタグとかを取っ払って文章だけに変換&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;$ echo &#39;gem &amp;quot;wp2txt&amp;quot;&#39; &amp;gt;&amp;gt; Gemfile
$ bundle
$ bundle exec wp2txt --input-file jawiki-latest-pages-articles.xml.bz
$ ls wp2txt/  # 変換結果
$ rm jawiki-latest-pages-articles.xml.bz # 不要だから消す
$ cat wp2txt/jawiki-latest-pages-articles-* &amp;gt; corpus.txt # 変換結果を一つのファイルに連結
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;とても時間かかります&amp;hellip;
記号除去とかした方が良いのかどうか、今のところ謎です。&lt;/p&gt;

&lt;h3 id=&#34;新語辞書使って分かち書き&#34;&gt;新語辞書使って分かち書き&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;$ mecab -Owakati -d /usr/local/lib/mecab/dic/mecab-ipadic-neologd corpus.txt &amp;gt; corpus_wakati.txt
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;とても時間かかります&amp;hellip;&lt;/p&gt;

&lt;h2 id=&#34;word2vecによるベクトル化&#34;&gt;word2vecによるベクトル化&lt;/h2&gt;

&lt;p&gt;今後pythonで実装することもあり、gensim使います。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ pip install gensim
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;utf-8に変換&#34;&gt;utf-8に変換&lt;/h3&gt;

&lt;p&gt;しないとword2vecのときに、文字コードについて怒られたので&amp;hellip;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ iconv -c -t UTF-8 &amp;lt; corpus_wakati.txt &amp;gt; corpus_wakati_utf-8.txt
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;学習&#34;&gt;学習&lt;/h3&gt;

&lt;p&gt;以下のpythonコードを実行します。次元は適当に中程度としました。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ vi word2vec_train.py
# coding: utf-8
from gensim.models import word2vec
import sys, logging, string, codecs

logging.basicConfig(format=&#39;%(asctime)s : %(levelname)s : %(message)s&#39;, level=logging.INFO)

# 学習（400次元だと４コアで約３時間...）
sentences = word2vec.Text8Corpus(&amp;quot;corpus_wakati_utf-8.txt&amp;quot;)
model = word2vec.Word2Vec(sentences, size=400, workers=4)
# モデルの保存
model.save(&amp;quot;w2v_model_%d_dims&amp;quot; % dims)
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;検証&#34;&gt;検証&lt;/h3&gt;

&lt;h4 id=&#34;モデルの読み込み&#34;&gt;モデルの読み込み&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;$ pyton
In [1]: from gensim.models import word2vec
In [2]: model = word2vec.Word2Vec.load(&amp;quot;./word2vec_models/w2v_model_%d_dims&amp;quot; % 400)
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;ベクトル空間上で近い単語を探す&#34;&gt;ベクトル空間上で近い単語を探す&lt;/h4&gt;

&lt;p&gt;動作確認です。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;In [3]: most_similar = model.most_similar(positive=[u&#39;サッカー&#39;])[0]
In [4]: most_similar[0]
ラグビー
In [5]: most_similar[1]
0.663492918015  # コサイン距離？
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;ベクトル取得&#34;&gt;ベクトル取得&lt;/h4&gt;

&lt;p&gt;このベクトルに対して今後処理していくことになります。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;In [1]: vector = model[u&#39;サッカー&#39;]
array([  2.69545317e-01,  -1.99663490e-01,   9.52050760e-02,
         2.16732353e-01,   1.97090670e-01,  -1.90409079e-01,
         ...
In [2]: vector.shape
(400,)
In [3]: vector.min()
-0.71880466
In [4]: vector.max()
0.75658286
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;ベクトルから単語を探す&#34;&gt;ベクトルから単語を探す&lt;/h4&gt;

&lt;p&gt;処理結果のベクトルから単語を復元する手段も確認しておきます。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;In [5]: vector[0:10] = 0.0  # 適当に変更
In [6]: for cname in [candidate[0] for candidate in model.most_similar(positive=[vector], topn=3)]:
            print cname
サッカー
ラグビー
フットサル
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;おわり&#34;&gt;おわり&lt;/h2&gt;

&lt;p&gt;ひとまず今回はここまで。単語をベクトル化したことにより、文章をベクトル時系列データとして扱うことが出来、色々な機械学習手法が適用可能になりました。
次回以降色々試してみたいと思います。&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&#34;おまけ&#34;&gt;おまけ&lt;/h1&gt;

&lt;p&gt;この時点でも色々作って遊べますね。例えば、以前作った絵文字サジェスト用hubotでは、キーワードに近しい、絵文字キーワードを引っ張ってきて候補を表示してました。Githubとかだと、標準でも絵文字キーワードを入力すると、続きをサジェストしてくれるものの、そもそものキーワードが思いつかなかったりするので、そういうときに便利です :)&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://blog.hassaku-labs.com/images/post/dialogue_system1/word2emoji.jpg&#34; alt=&#34;word2emoji.jpg&#34; /&gt;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>