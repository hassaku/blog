<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>hassaku&#39;s blog</title>
    <link>https://blog.hassaku-labs.com/</link>
    <description>Recent content on hassaku&#39;s blog</description>
    <generator>Hugo</generator>
    <language>ja</language>
    <lastBuildDate>Wed, 01 Jul 2020 00:00:00 +0900</lastBuildDate>
    <atom:link href="https://blog.hassaku-labs.com/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>ブログ引っ越し</title>
      <link>https://blog.hassaku-labs.com/post/move-blog/</link>
      <pubDate>Wed, 01 Jul 2020 00:00:00 +0900</pubDate>
      <guid>https://blog.hassaku-labs.com/post/move-blog/</guid>
      <description>&lt;p&gt;引越し先はこちら&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://hassaku-labs.hatenablog.com/&#34;&gt;https://hassaku-labs.hatenablog.com/&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>機械学習技術を使った実際の研究開発プロジェクトについて</title>
      <link>https://blog.hassaku-labs.com/post/machine-learning-project/</link>
      <pubDate>Sun, 05 May 2019 10:00:00 +0900</pubDate>
      <guid>https://blog.hassaku-labs.com/post/machine-learning-project/</guid>
      <description>&lt;p&gt;まずはざっくり箇条書き。もっと説明すべきところは、そのうち別の記事にするかも。&lt;/p&gt;&#xA;&lt;p&gt;あと本記事内容は、ある分野での機械学習案件における知見であって、そこまで汎用的なものではないかもしれない。&lt;/p&gt;&#xA;&lt;h1 id=&#34;全体的な雰囲気&#34;&gt;全体的な雰囲気&lt;/h1&gt;&#xA;&lt;p&gt;機械学習技術が必要になる仕事は全体の２割程度。よって、機械学習技術に精通していなくても活躍できる場面は多い。&#xA;むしろ、AutoMLや機械学習部分を自動化するようなフレームワークやツールが増えてきており、その他８割の方が今後は重要になるとも言える。&#xA;もちろん、その他作業を効率良く進めるためには、詳しいメンバーがいるに越したことはない。&lt;/p&gt;&#xA;&lt;h1 id=&#34;だいたいの流れ&#34;&gt;だいたいの流れ&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;解こうとしている課題の理解&#xA;&lt;ul&gt;&#xA;&lt;li&gt;本当に機械学習必要としているのかも早めに議論が必要&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;データの理解&#xA;&lt;ul&gt;&#xA;&lt;li&gt;可視化とか色々して仮説を立てる準備を整える&lt;/li&gt;&#xA;&lt;li&gt;この時点でゴミデータの存在には気づいておくことが大事&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;仮説の検討&#xA;&lt;ul&gt;&#xA;&lt;li&gt;人がちょっと考えて解ける問題は、入出力前提が同じであれば、（たぶん）機械学習で解ける&#xA;&lt;ul&gt;&#xA;&lt;li&gt;例）加速度センサーのデータを見て、走っているか歩いているかを判定する&lt;/li&gt;&#xA;&lt;li&gt;データが十分にあればDeep Learningを試しても良いだろう&lt;/li&gt;&#xA;&lt;li&gt;データが少なかったり、なんとなくルールが見えているような場合は、適切な特徴量エンジニアリングと判別器で解けるだろう&lt;/li&gt;&#xA;&lt;li&gt;仮説検討時の思考過程を参考にすること&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;人が考えてもなんだか分からない場合&#xA;&lt;ul&gt;&#xA;&lt;li&gt;例）加速度センサーのデータを見て、病気かどうか判定する&lt;/li&gt;&#xA;&lt;li&gt;他のデータを使えるか相談したり、そもそも病気ではないものを判定するタスクに置き換えられないか検討するといいかも&lt;/li&gt;&#xA;&lt;li&gt;機械学習精度どうこうの前に、そもそも実現性の検証が必要となり、だいたいコスパ悪い&#xA;&lt;ul&gt;&#xA;&lt;li&gt;一方で実現できたときは、優位性が認められることになるので、挑戦することは否定しない（置かれている状況次第）&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;データセットの準備&#xA;&lt;ul&gt;&#xA;&lt;li&gt;学習用、検証用、評価用&#xA;&lt;ul&gt;&#xA;&lt;li&gt;検証用、評価用データの抽出には時間をかけて良い&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;データ不足や偏り&#xA;&lt;ul&gt;&#xA;&lt;li&gt;モデル選択・パラメータチューニング用に評価用データを使うことがないように十分注意すること&#xA;&lt;ul&gt;&#xA;&lt;li&gt;検証用で色々最適化した結果、評価用でも良い結果。というのが期待される進捗&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;解こうとしているタスクに対して適切かどうか、見極めがとても大事&lt;/li&gt;&#xA;&lt;li&gt;適当に進めて間違っていた場合、このあとのプロセスが全て台無しになるだろう&#xA;&lt;ul&gt;&#xA;&lt;li&gt;適当に選んでいわゆるleakageがあったりすると悲惨&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;評価尺度の定義&#xA;&lt;ul&gt;&#xA;&lt;li&gt;ビジネス課題との整合性確認&lt;/li&gt;&#xA;&lt;li&gt;単なるF値やAUCを追い求めたら良いのか？あるいは、Precisionを重視してほしいのか等&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;評価系の構築&#xA;&lt;ul&gt;&#xA;&lt;li&gt;適切なデータセットと評価尺度が用意できれば、ほぼ終わったも同然（？）&lt;/li&gt;&#xA;&lt;li&gt;このあとはここまで色々頑張ってきた準備に対する、収穫の時期となるだろう&#xA;&lt;ul&gt;&#xA;&lt;li&gt;変な話、機械学習技術について精通した人が、機械学習未満のここまでをちゃんと設計出来ていると良いかも&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;特徴抽出の検討&#xA;&lt;ul&gt;&#xA;&lt;li&gt;あとで色々組み合わせたり、除外したくなるので、特徴量毎に個別CSV化しておくなどしておくと良い&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;機械学習技術の検討&#xA;&lt;ul&gt;&#xA;&lt;li&gt;ルールベースやロジスティック回帰などシンプルな手法で早めにベースラインを用意&lt;/li&gt;&#xA;&lt;li&gt;楽しい ٩( &amp;lsquo;ω&amp;rsquo; )و ♬*&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;検討した手法の評価&#xA;&lt;ul&gt;&#xA;&lt;li&gt;並列化とか早めにして、試行錯誤をすばやくする仕組みが大事&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;評価結果の課題分析&#xA;&lt;ul&gt;&#xA;&lt;li&gt;underfitting&#xA;&lt;ul&gt;&#xA;&lt;li&gt;表現能力を上げる。Deep Learningだったら層や素子数、その他だったら特徴量増やしたり&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;overfitting&#xA;&lt;ul&gt;&#xA;&lt;li&gt;正則化とかデータ増やしたりとか&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;具体的にうまくいかないデータを見たり、クラスタリング・可視化してみたりして、仮説を立てることはここでも大事&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;システム統合&#xA;&lt;ul&gt;&#xA;&lt;li&gt;精度を犠牲にしてでも、処理速度を上げたりしないといけないかも&amp;hellip;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;統合後の逐次評価&#xA;&lt;ul&gt;&#xA;&lt;li&gt;未知データの出現や分布、特徴の傾向が変化するかもしれないので継続的な改善は必要&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;各プロセスで、なにか壁にぶつかったら、必要なところまで戻り、を繰り返すのが機械学習リサーチエンジニアの主な仕事（たぶん）。&lt;/p&gt;&#xA;&lt;h1 id=&#34;進捗が芳しくない状況でありそうなケース&#34;&gt;進捗が芳しくない状況でありそうなケース&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;パラメータチューニングばかりやってる&lt;/li&gt;&#xA;&lt;li&gt;データクレンジングしてない&lt;/li&gt;&#xA;&lt;li&gt;課題を適切に分割してない&lt;/li&gt;&#xA;&lt;li&gt;解こうとしている問題のコスパが悪い&lt;/li&gt;&#xA;&lt;li&gt;仮説もってない&lt;/li&gt;&#xA;&lt;li&gt;トレードオフを行ったり来たりしてる&lt;/li&gt;&#xA;&lt;li&gt;etc&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;一時的なサポートメンバーに仕事を任せるために考えること&#34;&gt;一時的なサポートメンバーに仕事を任せるために考えること&lt;/h1&gt;&#xA;&lt;p&gt;いま一番頭を悩ましている部分&amp;hellip;&lt;/p&gt;</description>
    </item>
    <item>
      <title>データ分析コンペとかによくやる作業</title>
      <link>https://blog.hassaku-labs.com/post/data-comp-memo/</link>
      <pubDate>Fri, 03 May 2019 10:00:00 +0900</pubDate>
      <guid>https://blog.hassaku-labs.com/post/data-comp-memo/</guid>
      <description>&lt;h1 id=&#34;大きいデータの読み込み&#34;&gt;大きいデータの読み込み&lt;/h1&gt;&#xA;&lt;p&gt;daskを使って並列処理&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;import dask.dataframe as ddf&#xA;import dask.multiprocessing&#xA;&#xA;df = ddf.read_csv(&amp;#39;train_data.csv&amp;#39;)&#xA;df = df.compute(get=dask.multiprocessing.get)&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;h1 id=&#34;eda&#34;&gt;EDA&lt;/h1&gt;&#xA;&lt;p&gt;データに関する簡単な統計情報確認&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;import pandas_profiling as pdp&#xA;pdp.ProfileReport(df)&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;データの相関を確認&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;import seaborn as sns&#xA;sns.pairplot(df, hue=&amp;#34;target&amp;#34;, diag_kind=&amp;#34;kde&amp;#34;)&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;h1 id=&#34;前処理&#34;&gt;前処理&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;欠損値の補完&lt;/li&gt;&#xA;&lt;li&gt;外れ値の削除&lt;/li&gt;&#xA;&lt;li&gt;カテゴリ変数の扱い&#xA;&lt;ul&gt;&#xA;&lt;li&gt;one hot encoding （次元増える系）&#xA;&lt;ul&gt;&#xA;&lt;li&gt;どれかひとつだけ1になるようなスパースなベクトル表現&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;city_dummies = pd.get_dummies(df[&amp;#34;city&amp;#34;], prefix=&amp;#34;city&amp;#34;)&#xA;df.drop([&amp;#34;city&amp;#34;], axis=1, inplca=True)&#xA;df = df.join(city_dummies)&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;&#xA;&lt;li&gt;count encoding （次元増えない系）&#xA;&lt;ul&gt;&#xA;&lt;li&gt;カテゴリ変数の出現回数（あるいは率）を値とするような表現&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;df[&amp;#39;count_city&amp;#39;] = df.groupby(&amp;#39;city&amp;#39;)[&amp;#39;target&amp;#39;].transform(&amp;#39;count&amp;#39;)&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;&#xA;&lt;li&gt;並列処理&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;import numpy as np&#xA;import pandas as pd&#xA;#import multiprocessing as mp  # impossible to use lambda with pickle&#xA;import pathos.multiprocessing as mp  # dill is used inside instead of pickle.&#xA;&#xA;def split_parallel(df, num_split, map_func):&#xA;    with mp.Pool(num_split) as p:&#xA;        df_split = np.array_split(df, num_split*2)&#xA;        result = p.map(map_func, df_split)&#xA;    return pd.concat(result)&#xA;&#xA;NUM_PARALLELS = 3&#xA;df[&amp;#34;new_col&amp;#34;] = split_parallel(df, NUM_PARALLELS, lambda x: x[&amp;#34;col1&amp;#34;] + x[&amp;#34;col2&amp;#34;])&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;cv&#34;&gt;CV&lt;/h1&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;from sklearn.model_selection import StratifiedKFold&#xA;&#xA;cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=123)&#xA;X_train = np.zeros(20, 5)&#xA;y_train = np.zeros(20)&#xA;&#xA;for train_idx, valid_idx in cv.split(X_train, y_train):&#xA;    print(train_idx, valid_idx)&#xA;    ...&#xA;    losses.append(loss)&#xA;&#xA;cv_loss = np.mean(losses)&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;h1 id=&#34;結果のアンサンブル&#34;&gt;結果のアンサンブル&lt;/h1&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;res1 = pd.read_csv(&amp;#39;../method1/submission.csv&amp;#39;)&#xA;res2 = pd.read_csv(&amp;#39;../method2/submission.csv&amp;#39;)&#xA;res3 = pd.read_csv(&amp;#39;../method3/submission.csv&amp;#39;)&#xA;&#xA;b1 = res1.copy()&#xA;col = res1.columns&#xA;&#xA;col = col.tolist()&#xA;col.remove(&amp;#39;id&amp;#39;)&#xA;for i in col:&#xA;    b1[i] = (2 * res1[i]  + 2 * res2[i] + 4 * res3[i]) / 6.0&#xA;&#xA;b1.to_csv(&amp;#39;submission.csv&amp;#39;, index=False)&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;h1 id=&#34;jupyter-notebook上でcsvの確認とダウンロード&#34;&gt;Jupyter Notebook上でCSVの確認とダウンロード&lt;/h1&gt;&#xA;&lt;p&gt;実行するとディレクトリ内のファイル一覧が表示されて、クリックすればダウンロード出来るはず&lt;/p&gt;</description>
    </item>
    <item>
      <title>人工知能実現のための要素技術アイデア</title>
      <link>https://blog.hassaku-labs.com/post/ai-ideas/</link>
      <pubDate>Thu, 02 May 2019 10:00:00 +0900</pubDate>
      <guid>https://blog.hassaku-labs.com/post/ai-ideas/</guid>
      <description>&lt;p&gt;ただの夢想です。実現するには時間も考えも足りないので、忘れないうちに言語化しておきます。&lt;/p&gt;&#xA;&lt;h1 id=&#34;実現したいこと&#34;&gt;実現したいこと&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;コンセプト&#xA;&lt;ul&gt;&#xA;&lt;li&gt;心を感じるような人工的な知能を実現すること&#xA;&lt;ul&gt;&#xA;&lt;li&gt;条件反射ではない、なにか考えているような印象を与えてくれる&lt;/li&gt;&#xA;&lt;li&gt;常に学習し続けていて、変化を伴う&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;「疲れている」と伝えれば、それに共感してくれたり、自身のことを話してくれたりするイメージ&lt;/li&gt;&#xA;&lt;li&gt;知りたいことが返ってこなくても良い。むしろ何か見えないダイナミクスを感じさせてくれる挙動が大事&lt;/li&gt;&#xA;&lt;li&gt;知識の抽出だけではなく、知識の使い方をも自律的に発見する&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;シミュレーション&#xA;&lt;ul&gt;&#xA;&lt;li&gt;ユーザ（私）と体験をある程度共有出来るような環境&lt;/li&gt;&#xA;&lt;li&gt;グリッドワールドでも良い。その中に、時間や環境（温度や明るさ）、空腹感などの外的変動要素が含まれていること&lt;/li&gt;&#xA;&lt;li&gt;「今日は暑いね」と言えば、「暑いとは温度が比較的高いこと」など体験に基づいて内的に理解出来ていること&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;要素技術&#34;&gt;要素技術&lt;/h1&gt;&#xA;&lt;p&gt;情報表現も記憶も全ては動的であり、それを制御するための力学系を有すること&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;機構&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;基本は深層ニューラルネットのように徐々に抽象化されていく仕組み&lt;/li&gt;&#xA;&lt;li&gt;ただし層数は事前に決まらず、ニューロン同士の結合有無組み合わせ、再帰的な回路、時間発展によって、機能的に深層構造と同等の機能が動的に実現される&lt;/li&gt;&#xA;&lt;li&gt;結合の仕方を他のニューラルネットの状態によって制御出来れば、状況によって最適な構造（表現能力）を動的に用いることが出来る&lt;/li&gt;&#xA;&lt;li&gt;ある２つの情報が、同じカテゴリとして表現したいときもあれば、明確に区別したいときもあり、Attention機構（回路構造の指定）によって動的に制御される&lt;/li&gt;&#xA;&lt;li&gt;結合有無に関しては、MC Dropoutのようにある種のランダムを持たせて、いくらか学習が進んだあとに、エントロピー的計算により既知未知をある程度算出できる&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;学習方法&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;常に学習し続けても破綻しない&lt;/li&gt;&#xA;&lt;li&gt;誤差逆伝搬だと追加学習や並列学習に難があるので、Equilibrium Propagationのようなローカルな方法が望ましい&lt;/li&gt;&#xA;&lt;li&gt;誤差を求めるための正解値みたいなものは基本的に明示されず、自身の行動を決める上で、その情報表現が適切か不適切かを指標とする&lt;/li&gt;&#xA;&lt;li&gt;言い換えると、全ては時系列の予測学習であり、予測が出来るように学習していくことが基本的な戦略&#xA;&lt;ul&gt;&#xA;&lt;li&gt;時系列の先が同じであれば、表現は近くなるといった、CBOWとかskip-gramのイメージにも近い&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;記憶&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;大脳皮質の役割で、まだあまり議論されていないように思える&lt;/li&gt;&#xA;&lt;li&gt;hopfield型NNのようにアトラクタを有する力学系で実現される&#xA;&lt;ul&gt;&#xA;&lt;li&gt;追加学習をしても過去の記憶は壊れない。ただし、想起しづらくなることはある&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;AからBの予測を単に関数近似ではなく、神経力学系上の時間発展が可能とする連想記憶によって実現される&#xA;&lt;ul&gt;&#xA;&lt;li&gt;これによりA-&amp;gt;B、B-&amp;gt;Cを個別に学習しておくだけで、A-&amp;gt;Cの三段論法的な表現も自動的に獲得される&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;様々な角度の連想記憶は、力学系の部分空間の切り替えによって動的に制御される&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;探索&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;根底にあるのは欲求や感覚であり、持って生まれるものとする&lt;/li&gt;&#xA;&lt;li&gt;明示的なゴールは基本的に与えられない&lt;/li&gt;&#xA;&lt;li&gt;ご飯は食べれば食欲は満たされるポジティブな情報として、嫌なことは最終的に痛みに結びつきネガティブな情報として表現できる&lt;/li&gt;&#xA;&lt;li&gt;また、既知未知を表現できることで、知識欲を満たすように自律的な探索行動もできる&lt;/li&gt;&#xA;&lt;li&gt;そうやって、ネガティブなことを避けつつ、ポジティブなことを追い求めていく中で、徐々に外界情報が内部表現として整理されていく&lt;/li&gt;&#xA;&lt;li&gt;そうした中で、ユーザ（私）との共通認識みたいなものも生まれて、コミュニケーションも取れるようになるはず&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;書きかけであり、適宜追記するし、画像も用意する&lt;/p&gt;</description>
    </item>
    <item>
      <title>GASで作る日常ツールあれこれ</title>
      <link>https://blog.hassaku-labs.com/post/gas/</link>
      <pubDate>Wed, 01 May 2019 10:00:00 +0900</pubDate>
      <guid>https://blog.hassaku-labs.com/post/gas/</guid>
      <description>&lt;h1 id=&#34;基本google-apps-scriptの作り方&#34;&gt;【基本】Google Apps Scriptの作り方&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;スプレッドシート作成&lt;/li&gt;&#xA;&lt;li&gt;ツール - スクリプトエディタ&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;基本webapi化の共通事項&#34;&gt;【基本】WebAPI化の共通事項&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;doPostという関数&lt;/li&gt;&#xA;&lt;li&gt;公開 &amp;gt; webアプリケーションとして導入 を選び公開&lt;/li&gt;&#xA;&lt;li&gt;コードを修正した場合、バージョン に 新規作成 を選択する必要がある。&lt;/li&gt;&#xA;&lt;li&gt;アクセス出来るユーザは全員（匿名）にしておくこと&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;基本簡単なデータストア先として活用&#34;&gt;【基本】簡単なデータストア先として活用&lt;/h1&gt;&#xA;&lt;p&gt;WiFi接続のセンサモジュールとか、ちょっとしたものからデータを記録していきたいときに利用&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;var sheet = SpreadsheetApp.openById(URLのところに表示されるID).getSheetByName(シート名);&#xA;&#xA;function doPost(e) {&#xA;  var array = [e.parameter.timestamp, e.parameter.sensor_id, e.parameter.value];&#xA;  sheet.appendRow(array);&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;$ curl -X POST -F &amp;#34;timestamp=`date &amp;#34;+%Y%m%d %H:%M:%S&amp;#34;`&amp;#34; -F &amp;#39;sensor_id=1234&amp;#39; -F &amp;#39;value=5678&amp;#39; 公開時に表示されるURL&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;スプレッドシートにデータが追記されていくので、それをCSV化してデータ分析するなり、簡単に可視化するなり。&lt;/p&gt;&#xA;&lt;h1 id=&#34;基本slack通知の共通部分&#34;&gt;【基本】Slack通知の共通部分&lt;/h1&gt;&#xA;&lt;p&gt;以下の各事例でも頻繁に使われてる&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;/*&#xA;リソース - ライブラリから以下を追加&#xA;- SlackApp: M3W5Ut3Q39AaIwLquryEPMwV62A3znfOO&#xA;- Underscore: MGwgKN2Th03tJ5OdmlzB8KPxhMjh3Sh48&#xA;- Moment: MHMchiX6c1bwSqGM1PZiW_PxhMjh3Sh48&#xA;*/&#xA;&#xA;var _ = Underscore.load();&#xA;var TOKEN = &amp;#34;SLACK_TOKEN&amp;#34;;&#xA;&#xA;var slackApp = SlackApp.create(TOKEN);&#xA;&#xA;function getChannelId(name) {&#xA;  // チャンネル名から通知に必要なIDを取得&#xA;  var channel = _.findWhere(slackApp.channelsList().channels, {name: name});&#xA;  if (_.isEmpty(channel)) {&#xA;    throw new Error(name + &amp;#34; is not found&amp;#34;);&#xA;  }&#xA;  return channel.id&#xA;}&#xA;&#xA;function postMessage(channel_name, message) {&#xA;  // 任意のメッセージを通知&#xA;  var channelId = getChannelId(slackApp, channel_name);&#xA;&#xA;  slackApp.chatPostMessage(channelId, message, {&#xA;    username : &amp;#34;bot&amp;#34;,&#xA;    icon_emoji : &amp;#34;:mega:&amp;#34;&#xA;  });&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;h1 id=&#34;googleドキュメントスライド定期複製&#34;&gt;Googleドキュメント・スライド定期複製&lt;/h1&gt;&#xA;&lt;p&gt;定例ミーティングなどで、毎回人手で過去分議事録をコピーして、事前メモ用ドキュメントを作成しているケースがあったので自動化した。&#xA;コピー元は、前回分とかではなく、毎回決まったテンプレートとかを指定しても良い。&lt;/p&gt;</description>
    </item>
    <item>
      <title>IDE使えないときの例外時デバッグ</title>
      <link>https://blog.hassaku-labs.com/post/debug-on-remote/</link>
      <pubDate>Mon, 25 Feb 2019 10:00:00 +0900</pubDate>
      <guid>https://blog.hassaku-labs.com/post/debug-on-remote/</guid>
      <description>&lt;p&gt;IDEが使える状況であれば、簡単にデバッガーを使えると思うが、そうでない環境でデバッグしたくなることもある。&#xA;printデバッグも辛いので、以下のような感じで例外時等にpython debuggerを起動出来ると便利。&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;import sys&#xA;&#xA;def hook(type, value, tb):&#xA;   if hasattr(sys, &amp;#39;ps1&amp;#39;) or not sys.stderr.isatty():&#xA;      sys.__excepthook__(type, value, tb)&#xA;   else:&#xA;      import traceback, pdb&#xA;      traceback.print_exception(type, value, tb)&#xA;      print()&#xA;      pdb.pm()&#xA;&#xA;sys.excepthook = hook&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;というのを適当なところにdebug.pyとして置いておいて、デバッグしたいコード実行時にimportしておくだけ。&#xA;例外が起きるとデバッガが起動し停止するので、そのときの変数の値などを表示したり調べると良い。&lt;/p&gt;&#xA;&lt;p&gt;不必要に毎回デバッガ起動すると面倒なので、普段はimport行をコメントアウトしておくこと。&lt;/p&gt;&#xA;&lt;p&gt;よく使うコマンドは以下のとおり。&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;w&#xA;&lt;ul&gt;&#xA;&lt;li&gt;where. スタックトレースを表示する。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;u&#xA;&lt;ul&gt;&#xA;&lt;li&gt;up. スタックを一つ上に移動&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;d&#xA;&lt;ul&gt;&#xA;&lt;li&gt;down. スタックを一つ下に移動&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;l [first, last]&#xA;&lt;ul&gt;&#xA;&lt;li&gt;list. 現在のソースコード表示。fist, lastを指定可能。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;p [target]&#xA;&lt;ul&gt;&#xA;&lt;li&gt;print. 変数等を表示。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;c&#xA;&lt;ul&gt;&#xA;&lt;li&gt;continue. 次のブレークポイントに当たるまで実行&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;s&#xA;&lt;ul&gt;&#xA;&lt;li&gt;step. 現在の行を実行 (関数の中に入る）&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;n&#xA;&lt;ul&gt;&#xA;&lt;li&gt;next. 現在の行を実行 (関数の中に入らない）&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;q&#xA;&lt;ul&gt;&#xA;&lt;li&gt;quit. デバッガを終了&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;</description>
    </item>
    <item>
      <title>word vectorのような読み込みが重たいやつをWebAPI化して軽量化</title>
      <link>https://blog.hassaku-labs.com/post/word-vector-api/</link>
      <pubDate>Mon, 25 Feb 2019 10:00:00 +0900</pubDate>
      <guid>https://blog.hassaku-labs.com/post/word-vector-api/</guid>
      <description>&lt;p&gt;word vectorとかメモリをどデカく使うようなやつは、毎回スクリプトを起動する際に読み込みに時間がかかって辛い。&#xA;そういうのは、極力別のプロセスにして、適当にAPIとか生やして連携するようにしておくと楽チンなので良くやるパターン。&lt;/p&gt;&#xA;&lt;p&gt;以下は、単語ベクトルを返してくれるAPIを作った例。Flask使うとコードもシンプルに実現出来るので良い。&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;# coding: utf-8&#xA;&#xA;import numpy as np&#xA;import gensim&#xA;from flask import Flask, jsonify, request&#xA;import json&#xA;&#xA;PRETRAINED_W2V_PATH = &amp;#39;./model.bin&amp;#39;&#xA;&#xA;app = Flask(__name__)&#xA;app.config[&amp;#39;JSON_AS_ASCII&amp;#39;] = False&#xA;&#xA;model = gensim.models.KeyedVectors.load_word2vec_format(PRETRAINED_W2V_PATH, binary=True)  # 超時間かかる処理&#xA;&#xA;@app.route(&amp;#39;/word_vector&amp;#39;, methods=[&amp;#39;GET&amp;#39;])&#xA;def word_vector():&#xA;    word = request.args.get(&amp;#39;word&amp;#39;)&#xA;    vector = np.array(model[word]).astype(float).tolist()&#xA;    return jsonify({&amp;#39;vector&amp;#39;: vector}), 200&#xA;&#xA;&#xA;if __name__ == &amp;#34;__main__&amp;#34;:&#xA;    app.debug = True&#xA;    app.run(host=&amp;#39;0.0.0.0&amp;#39;, port=8888)&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;以下のような感じで単語ベクトルの値をjsonで返してくれる。pythonのスクリプトからはrequestsとかで簡単に取得して扱えるはず。&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;$ curl &amp;#34;http://0.0.0.0:8888/word_vector?word=テスト&amp;#34;&#xA;{&#xA;  &amp;#34;vector&amp;#34;: [&#xA;    0.029713749885559082,&#xA;    -0.6024296283721924,&#xA;    0.9723357558250427,&#xA;    -1.1497808694839478,&#xA;    1.3764394521713257,&#xA;...&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;例えば、連続動作しているようなエージェントシミュレータなんかにも、似たような感じでAPI生やして、インタラクションさせることが出来る。&lt;/p&gt;</description>
    </item>
    <item>
      <title>ターミナル上でシンプルなグリッドワールド</title>
      <link>https://blog.hassaku-labs.com/post/grid-world/</link>
      <pubDate>Fri, 10 Aug 2018 10:00:00 +0900</pubDate>
      <guid>https://blog.hassaku-labs.com/post/grid-world/</guid>
      <description>&lt;p&gt;強化学習などでグリッドワールドを使いたいとき、gym-minigridとかpycolabがあるけど、色々いじる必要性もある場合、もっとシンプルなところからはじめたい。&#xA;また、リモートのVMインスタンス上などで気軽に動かしたいので、GUIとかも無しで、ターミナル上で動かしたい。&lt;/p&gt;&#xA;&lt;p&gt;以下のような感じで、cursesを使ってスクラッチで作っても別に難しいことはなかった。&lt;/p&gt;&#xA;&lt;p&gt;こんな感じのやつがターミナル上で動く。&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;https://blog.hassaku-labs.com/images/post/grid-world/grid-world.gif&#34; alt=&#34;grid-world&#34;&gt;&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;import curses&#xA;import random&#xA;import time&#xA;from datetime import datetime&#xA;&#xA;FIELD = [&amp;#39;#################&amp;#39;,&#xA;         &amp;#39;#       #       #&amp;#39;,&#xA;         &amp;#39;#       #       #&amp;#39;,&#xA;         &amp;#39;#       #       #&amp;#39;,&#xA;         &amp;#39;#               #&amp;#39;,&#xA;         &amp;#39;#       #       #&amp;#39;,&#xA;         &amp;#39;######  #########&amp;#39;,&#xA;         &amp;#39;#       #       #&amp;#39;,&#xA;         &amp;#39;#       #       #&amp;#39;,&#xA;         &amp;#39;#               #&amp;#39;,&#xA;         &amp;#39;#       #       #&amp;#39;,&#xA;         &amp;#39;#       #       #&amp;#39;,&#xA;         &amp;#39;#################&amp;#39;]&#xA;&#xA;&#xA;def draw(screen):&#xA;    for row, line in enumerate(FIELD):&#xA;        for col, tile in enumerate(line):&#xA;            screen.addch(row, col, tile)&#xA;&#xA;&#xA;def main():&#xA;    x = 10&#xA;    y = 10&#xA;&#xA;    try:&#xA;        screen = curses.initscr()&#xA;        screen.nodelay(1)&#xA;        curses.curs_set(0)&#xA;&#xA;        while(True):&#xA;            action = random.randint(1, 5)&#xA;            dx = 0&#xA;            dy = 0&#xA;            if action == 1:&#xA;                dy += 1&#xA;            elif action == 2:&#xA;                dy -= 1&#xA;            elif action == 3:&#xA;                dx += 1&#xA;            elif action == 4:&#xA;                dx -= 1&#xA;            elif action == 5:&#xA;                pass&#xA;            else:&#xA;                raise NotImplementedError()&#xA;&#xA;            # check wall&#xA;            if FIELD[x + dx][y + dy] != &amp;#34;#&amp;#34;:&#xA;                x += dx&#xA;                y += dy&#xA;&#xA;            screen.clear()&#xA;            draw(screen)&#xA;            screen.addch(x, y, &amp;#39;+&amp;#39;) # agent&#xA;&#xA;            screen.addstr(0, 20, datetime.now().strftime(&amp;#34;%Y/%m/%d %H:%M:%S&amp;#34;))&#xA;            screen.addstr(1, 20, &amp;#39;a:{} x:{} y:{}&amp;#39;.format(action, x, y))&#xA;            screen.refresh()&#xA;&#xA;            # quit&#xA;            if(screen.getch() == ord(&amp;#39;q&amp;#39;)):&#xA;                break&#xA;&#xA;            time.sleep(0.2)&#xA;&#xA;        curses.endwin()&#xA;&#xA;    except:&#xA;        pass&#xA;&#xA;    finally:&#xA;        curses.echo()&#xA;        curses.endwin()&#xA;&#xA;&#xA;if __name__ == &amp;#39;__main__&amp;#39;:&#xA;    main()&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;もしエージェントとインタラクションしたいと思ったら、flaskとかでapi作って状態変えるのが良いと思う。やり方はまた別の機会に。&lt;/p&gt;</description>
    </item>
    <item>
      <title>単語ベクトルと全結合ニューラルネットワークによる単語連想記憶</title>
      <link>https://blog.hassaku-labs.com/post/associative-word-vector/</link>
      <pubDate>Sun, 15 Jul 2018 00:00:00 +0900</pubDate>
      <guid>https://blog.hassaku-labs.com/post/associative-word-vector/</guid>
      <description>&lt;p&gt;自然言語処理にニューラルネットワークを適用する事例が増えている。&#xA;ここでは、従来の部分的に再帰結合をもつようなRNN(LSTM)といった構造ではなく、&#xA;(最近はあまり流行っていない）ホップフィールドモデルのような全結合構造のニューラルネットを用いて、&#xA;エネルギーポテンシャルの窪み、引き込み領域を有するアトラクタ空間に言語知識を記憶させることを目指す。&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;https://blog.hassaku-labs.com/images/post/associative-word-vector/potential.png&#34; alt=&#34;potential&#34;&gt;&lt;/p&gt;&#xA;&lt;p&gt;メリットとして、以下のようなことが挙げられる。&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;追加学習が容易&lt;/li&gt;&#xA;&lt;li&gt;データを大量に学習しなくても汎化性能が高い&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;いくつか想定している内容のうち、今回は単語ベクトルのペアを連想記憶する基本的なタスクを検証する。&lt;/p&gt;&#xA;&lt;h1 id=&#34;検証用コード&#34;&gt;検証用コード&lt;/h1&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/hassaku/associative-word-vector/tree/blog-1&#34;&gt;https://github.com/hassaku/associative-word-vector/tree/blog-1&lt;/a&gt;&lt;/p&gt;&#xA;&lt;h1 id=&#34;検証内容&#34;&gt;検証内容&lt;/h1&gt;&#xA;&lt;p&gt;単語ベクトルは、Github上のREADMEに書いてあるように、東北大学 乾・岡崎研究室で公開されている学習済みのものを利用させて頂いた。&lt;/p&gt;&#xA;&lt;p&gt;タスクについては、以下に示すような「鳩」(que) → 「飛ぶ」(target) といった単純な知識を学習させることにする。&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;que target&#xA; 鳩   飛ぶ&#xA; 牛   走る&#xA; 鯉   泳ぐ&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;その後、以下のような未学習の単語(que)に対し、学習済みの知識に基づき、適切な単語(target)を想起出来るかどうかを確かめる。&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;   que target&#xA; カラス   飛ぶ&#xA;    馬   走る&#xA;    鯛   泳ぐ&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;期待される結果としては、「カラス」も「鳩」と同じく鳥であることから、「飛ぶ」と連想されること。&#xA;原理的には、図のような線上のアトラクタに知識を埋め込むことにより、学習済みのもの(que1)に親しい単語ベクトル(que1&amp;rsquo;)であれば、&#xA;引き込まれてtarget1を記憶させた平衡点に至ることで想起が実現される。&lt;/p&gt;&#xA;&lt;p&gt;今回は原理確認のため、以下のように事前に単語ベクトル間の近さ（コサイン類似度）を確認した上でタスク設定している。&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;  word    カラス      泳ぐ        牛      走る      飛ぶ        馬        鯉        鯛        鳩&#xA;カラス  1.000000  0.395338  0.475648  0.162681  0.493993  0.230542  0.406434  0.324465  0.670461&#xA;  泳ぐ  0.395338  1.000000  0.166645  0.576415  0.681419  0.162183  0.277283  0.179173  0.385671&#xA;    牛  0.475648  0.166645  1.000000  0.083026  0.149580  0.589086  0.559115  0.564100  0.565433&#xA;  走る  0.162681  0.576415  0.083026  1.000000  0.508805  0.123983 -0.021091 -0.010774  0.193441&#xA;  飛ぶ  0.493993  0.681419  0.149580  0.508805  1.000000  0.110546  0.133639  0.123491  0.422164&#xA;    馬  0.230542  0.162183  0.589086  0.123983  0.110546  1.000000  0.251400  0.283276  0.281816&#xA;    鯉  0.406434  0.277283  0.559115 -0.021091  0.133639  0.251400  1.000000  0.697661  0.617204&#xA;    鯛  0.324465  0.179173  0.564100 -0.010774  0.123491  0.283276  0.697661  1.000000  0.533510&#xA;    鳩  0.670461  0.385671  0.565433  0.193441  0.422164  0.281816  0.617204  0.533510  1.000000&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;感覚として近そうな単語を選んだつもりでも、意外と単語ベクトルとしては離れていたりする。&#xA;そういう場合は、意図しないアトラクタに引き寄せられてしまうため、注意が必要である。&#xA;単語ベクトルは、文章中の使われ方を想定して学習されているため、そういうこともあるかとは思うが、&#xA;本来は文脈によって、単語間の距離も適切に遠近するはずである。&#xA;それについては、次回以降検証することにしたい。&lt;/p&gt;</description>
    </item>
    <item>
      <title>ターミナル上で１行の簡易グラフ</title>
      <link>https://blog.hassaku-labs.com/post/ascii-simple-plot/</link>
      <pubDate>Mon, 23 Apr 2018 00:00:00 +0900</pubDate>
      <guid>https://blog.hassaku-labs.com/post/ascii-simple-plot/</guid>
      <description>&lt;p&gt;データ処理などしていると、処理中の状況を確認するのに、数値や文字列よりもグラフの方が適していることが多い。&#xA;ただ、そのためだけにGUIなどを用意するのは大変だし、出来ればターミナル上で表示したい。&#xA;というわけで、ログなど混ぜて、以下のような感じで、簡易的にグラフ描くと便利。&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;$ cat simple_plot.py&#xA;&#xA;# coding: utf-8&#xA;&#xA;import numpy as np&#xA;&#xA;TICKS = u&amp;#39;_▁▂▃▄▅▆▇█&amp;#39;&#xA;&#xA;def ascii_plot(ints, max_range=None, min_range=0, width=40):&#xA;    assert len(ints) &amp;gt;= width&#xA;&#xA;    ints = np.array(ints, dtype=int)&#xA;    ints = ints[:int(len(ints)/width)*width]&#xA;    ints = np.nansum(np.reshape(ints, (int(len(ints)/width), width)).T, axis=1)&#xA;&#xA;    if not max_range:&#xA;        max_range = max(ints)&#xA;    if not min_range:&#xA;        min_range = min(ints)&#xA;&#xA;    step_range = max_range - min_range&#xA;    step = (step_range / float(len(TICKS) - 1)) or 1&#xA;    return u&amp;#39;&amp;#39;.join(TICKS[int(round((i - min_range) / step))] for i in ints)&#xA;&#xA;&#xA;if __name__ == &amp;#34;__main__&amp;#34;:&#xA;   print(ascii_plot(range(10) + range(10, 0, -1), width=20))&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;$ python simple_plot.py&#xA;_▁▂▂▃▄▅▆▆▇█▇▆▆▅▄▃▂▂▁&#xA;&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    <item>
      <title>pythonの各種使い方をオフラインで確認する方法</title>
      <link>https://blog.hassaku-labs.com/post/python-offline-help/</link>
      <pubDate>Tue, 20 Feb 2018 00:00:00 +0900</pubDate>
      <guid>https://blog.hassaku-labs.com/post/python-offline-help/</guid>
      <description>&lt;p&gt;インターネットから隔離されていたりや書籍が持ち込みできないような環境で、プログラミングをすることがあった。&#xA;こういうときのために、各種ライブラリの使い方などをオフラインでも確認できるようにしておくと良い。&lt;/p&gt;&#xA;&lt;h2 id=&#34;各種ライブラリのヘルプ確認の仕方&#34;&gt;各種ライブラリのヘルプ確認の仕方&lt;/h2&gt;&#xA;&lt;p&gt;例えば、scikit-learnのkmeansクラスタリングの使い方を調べたいとき。&#xA;一番上から順に探していく。&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;&amp;gt; import sklearn&#xA;&amp;gt; help(sklearn)&#xA;...&#xA;PACKAGE CONTENTS&#xA;    ...&#xA;    cluster (package)&#xA;...&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;&amp;gt; import sklearn.cluster&#xA;&amp;gt; help(sklearn.cluster)&#xA;...&#xA;PACKAGE CONTENTS&#xA;    ...&#xA;    k_means_&#xA;...&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;&amp;gt; import sklearn.cluster.k_means_&#xA;&amp;gt; help(sklearn.cluster.k_means_)&#xA;class KMeans(sklearn.base.BaseEstimator, sklearn.base.ClusterMixin, sklearn.base.TransformerMixin)&#xA;     |  K-Means clustering&#xA;     |&#xA;     |  Parameters&#xA;...&#xA;目的の使い方にたどり着いた&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;jupyter-notebook上でのやり方&#34;&gt;jupyter notebook上でのやり方&lt;/h2&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;&amp;gt; import pandas as pd&#xA;&amp;gt; pd.read_csv?&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;下部にヘルプ用のウインドウが別途表示されるので、若干見やすい（？）&lt;/p&gt;</description>
    </item>
    <item>
      <title>Slackのメッセージ収集</title>
      <link>https://blog.hassaku-labs.com/post/slack-mining/</link>
      <pubDate>Tue, 23 Jan 2018 00:00:00 +0900</pubDate>
      <guid>https://blog.hassaku-labs.com/post/slack-mining/</guid>
      <description>&lt;p&gt;Slackに投稿されたメッセージを収集する方法についてのメモ&lt;/p&gt;&#xA;&lt;h2 id=&#34;データexport&#34;&gt;データexport&lt;/h2&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://xxxx.slack.com/services/export&#34;&gt;https://xxxx.slack.com/services/export&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;exportが完了すると、slack上でbotから通知くる&lt;/p&gt;&#xA;&lt;h2 id=&#34;ユーザid取得&#34;&gt;ユーザID取得&lt;/h2&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;$ curl https://slack.com/api/users.list\?token\=YOUR_SLACK_TOKEN&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;最新のSlack Token取得方法は色々記事が挙がっているのでググること&lt;/p&gt;&#xA;&lt;h2 id=&#34;一ヶ月のユーザの発言を収集する&#34;&gt;一ヶ月のユーザの発言を収集する&lt;/h2&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;$ cat show_messages.rb&#xA;&#xA;require &amp;#39;json&amp;#39;&#xA;&#xA;user = &amp;#34;USER_ID&amp;#34;&#xA;year = 2016&#xA;month = 2&#xA;days = 29&#xA;&#xA;days.times do |day|&#xA;  date = &amp;#34;%d-%2d-%02d&amp;#34; % (year, month, day + 1)&#xA;  export_file = &amp;#34;./#{date}.json&amp;#34;&#xA;  next unless File.exist?(export_file)&#xA;  puts &amp;#34;\n----- #{date} -----&amp;#34;&#xA;&#xA;  json_data = open(export_file) do |io|&#xA;    JSON.load(io)&#xA;  end&#xA;&#xA;  json_data.each do |json|&#xA;    puts &amp;#34;#{Time.at(json[&amp;#39;ts&amp;#39;].to_i)}: #{json[&amp;#39;text&amp;#39;].gsub(/\n+/, &amp;#39; &amp;#39;)}&amp;#34; if json[&amp;#34;user&amp;#34;] == user&#xA;  end&#xA;end&#xA;&#xA;$ ruby show_messages.rb&#xA;&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    <item>
      <title>partial AUCでprecision重視の評価</title>
      <link>https://blog.hassaku-labs.com/post/partial-auc/</link>
      <pubDate>Sat, 23 Dec 2017 00:59:39 +0900</pubDate>
      <guid>https://blog.hassaku-labs.com/post/partial-auc/</guid>
      <description>&lt;p&gt;機械学習の判別器を評価する際、F値やAUCはよく使われていると思います。&#xA;しかしながら、実応用の分野によっては、検出出来ないこと(false negative)よりも、誤検出(false positive)の方が問題視されることがよくあります。&#xA;そういったprecision重視にしたいケースで使える指標がpartial AUCです。&#xA;一般的には、false positiveに制約をもたせて用いるようです。AUCの面積を求める際に、そのfalse positive rateの下限値以上の部分のみ使います。&lt;/p&gt;&#xA;&lt;p&gt;簡単に実装するため、scikit learnのaucのコードをオーバーライドして使っています。&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;import numpy as np&#xA;from sklearn.metrics import roc_auc_score, roc_curve, auc&#xA;from sklearn.metrics.base import _average_binary_score&#xA;&#xA;# method overriding&#xA;def roc_auc_score(y_true, y_score, average=&amp;#34;macro&amp;#34;, sample_weight=None, max_fpr=None):&#xA;    def _binary_roc_auc_score(y_true, y_score, sample_weight=None, max_fpr=max_fpr):&#xA;        fpr, tpr, tresholds = roc_curve(y_true, y_score, sample_weight=sample_weight)&#xA;&#xA;        if max_fpr:&#xA;            idx = np.where(fpr &amp;lt;= max_fpr)[0]&#xA;&#xA;            idx_last = idx.max()&#xA;            idx_next = idx_last + 1&#xA;            xc = [fpr[idx_last], fpr[idx_next]]&#xA;            yc = [tpr[idx_last], tpr[idx_next]]&#xA;            tpr = np.r_[tpr[idx], np.interp(max_fpr, xc, yc)]&#xA;            fpr = np.r_[fpr[idx], max_fpr]&#xA;            partial_roc = auc(fpr, tpr, reorder=True)&#xA;&#xA;            # standardize result to lie between 0.5 and 1&#xA;            min_area = max_fpr**2/2&#xA;            max_area = max_fpr&#xA;            return 0.5*(1+(partial_roc-min_area)/(max_area-min_area))&#xA;&#xA;        return auc(fpr, tpr, reorder=True)&#xA;&#xA;    return _average_binary_score(_binary_roc_auc_score, y_true, y_score, average, sample_weight=sample_weight)&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;以下のような感じで試してみると、AUCが同じスコアであっても、precisionが高い方がpAUCでは比較的高い値になることが分かります。&#xA;よって、pAUCを指標に学習やパラメータチューニングを行えば、precision重視のものが出来上がります。&lt;/p&gt;</description>
    </item>
    <item>
      <title>自分流論文メモのやり方</title>
      <link>https://blog.hassaku-labs.com/post/paper-management/</link>
      <pubDate>Fri, 22 Dec 2017 17:17:30 +0900</pubDate>
      <guid>https://blog.hassaku-labs.com/post/paper-management/</guid>
      <description>&lt;p&gt;普段から興味ある分野について、arXivを漁っている人多いと思うのですが、やはり手っ取り早く内容を俯瞰するには、abstractが日本語になっている方が負荷が少ないと思います。日本語の概要を一瞬見て、ちゃんと読んだ方がいいかどうか判断する感じです。&lt;/p&gt;&#xA;&lt;p&gt;自分は以下のような感じで、arxivのidを貼り付けると、自動でタイトルとか概要を取得して、隣に翻訳も表示してくれるようなスプレッドシートを用意しているのですが、便利なので公開したいと思います。ついでに、実装が公開されていればそのリンクや、簡単な感想なんかもメモするようにしています。&lt;/p&gt;&#xA;&lt;!-- raw HTML omitted --&gt;&#xA;&lt;p&gt;以下から適当にコピーして頂ければ、使えるかと思います。&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://docs.google.com/spreadsheets/d/19AHvw82bFWivJtoVTohvUDs-uMuhyH0Dw0IzIpvviog&#34;&gt;https://docs.google.com/spreadsheets/d/19AHvw82bFWivJtoVTohvUDs-uMuhyH0Dw0IzIpvviog&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;誰かのお役に立てれば幸いです。。&lt;/p&gt;</description>
    </item>
    <item>
      <title>hyperoptでパラメータサーチ</title>
      <link>https://blog.hassaku-labs.com/post/hyperopt/</link>
      <pubDate>Sat, 04 Mar 2017 04:08:51 +0900</pubDate>
      <guid>https://blog.hassaku-labs.com/post/hyperopt/</guid>
      <description>&lt;p&gt;機械学習におけるモデルのハイパーパラメータ探索は、その後のモデル評価を正当に行うことにも繋がる、重要な作業です。&#xA;近年ではRandom SearchやGrid Search、GA等の各種最適化手法よりも、色々と優れた手法が提案されているらしく、&#xA;手軽に使えるようなライブラリも整備されています。&#xA;パラメータ探索技術というと、応用範囲が広く、効果が見えやすいため、手軽に使えて効率的な手法があれば、積極的に使っていきたいところです。&#xA;その中でもhyperoptというライブラリが、kaggleとかでよく使われているという話を見かけて、試しに使ってみました。&lt;/p&gt;&#xA;&lt;p&gt;中身については、色々Blogや論文が見つかるのですが、&#xA;Bayesian Optimization -&amp;gt; Sequential Model-based Algorithm Configuration (SMAC) -&amp;gt; Tree-structured Parzen Estimator (TPE)&#xA;のように進化してきたTPEという手法が使われているようです。&#xA;Bayesian Optimizationのアプローチは、直感的にも効率良さそうなので、その進化系なら期待できます。&lt;/p&gt;&#xA;&lt;p&gt;[Hyperopt]&#xA;(&lt;a href=&#34;http://hyperopt.github.io/hyperopt/&#34;&gt;http://hyperopt.github.io/hyperopt/&lt;/a&gt;)&lt;/p&gt;&#xA;&lt;h2 id=&#34;簡単な使い方&#34;&gt;簡単な使い方&lt;/h2&gt;&#xA;&lt;p&gt;以下のような感じで、簡単に記述出来て、手軽に取り入れられそうです。&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;#!/usr/bin/env python&#xA;# encoding: utf-8&#xA;&#xA;import os&#xA;import sys&#xA;&#xA;import matplotlib.pyplot as plt&#xA;import numpy as np&#xA;from hyperopt import fmin, tpe, hp, Trials&#xA;&#xA;# パラメータの探索範囲。指定方法は離散値や連続値などに応じて色々ある。&#xA;# https://github.com/hyperopt/hyperopt/wiki/FMin#21-parameter-expressions&#xA;hyperopt_parameters = { &amp;#39;x&amp;#39;: hp.uniform(&amp;#39;x&amp;#39;, -30, 30) }&#xA;&#xA;# 最小化したい目的関数。正解との誤差とか。&#xA;def objective(args):&#xA;    return np.sin(args[&amp;#39;x&amp;#39;]) / args[&amp;#39;x&amp;#39;]&#xA;&#xA;# 探索中の進行状況を記録&#xA;trials = Trials()&#xA;&#xA;# パラメータサーチの実行&#xA;best = fmin(objective,  hyperopt_parameters, algo=tpe.suggest, max_evals=300, trials=trials)&#xA;&#xA;# 目的関数を最小にするパラメータ&#xA;print best&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;パラメータサーチの様子を表示&#34;&gt;パラメータサーチの様子を表示&lt;/h2&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;# 上記コードの続き&#xA;&#xA;x_hisotry = np.ravel([t[&amp;#39;misc&amp;#39;][&amp;#39;vals&amp;#39;][&amp;#39;x&amp;#39;] for t in trials.trials])&#xA;objective_history = trials.losses()&#xA;&#xA;fig = plt.figure(figsize=(16, 8))&#xA;cm = plt.get_cmap(&amp;#39;jet&amp;#39;)&#xA;&#xA;PLOT_NUM = 5&#xA;&#xA;for i, hist_num in enumerate(np.linspace(50, 300, PLOT_NUM)):&#xA;    cmap_cycle = [cm(1. * h/ (hist_num - 1)) for h in range(int(hist_num) - 1)]&#xA;&#xA;    ax = plt.subplot(2, PLOT_NUM, i + 1)&#xA;    ax.set_color_cycle(cmap_cycle)&#xA;    ax.plot(np.arange(-30, 30, 0.1), function(np.arange(-30, 30, 0.1)), alpha=0.2)&#xA;    for j in range(int(hist_num)  - 1):&#xA;        ax.plot(x_hisotry[j], objective_history[j], &amp;#39;.&amp;#39;)&#xA;    ax.set_title(&amp;#39;times: {times}&amp;#39;.format(times=int(hist_num)))&#xA;    ax.set_ylim([np.min(objective_history) - 0.1, np.max(objective_history) + 0.1])&#xA;    ax.set_xlim([-30, 30])&#xA;    if i == 0:&#xA;        ax.set_ylabel(&amp;#39;y&amp;#39;)&#xA;&#xA;    plt.subplot(2, PLOT_NUM, PLOT_NUM + i + 1)&#xA;    plt.hist(x_hisotry[:hist_num], bins=50)&#xA;    if i == 0:&#xA;        plt.ylabel(&amp;#39;histogram of x&amp;#39;)&#xA;    plt.xlabel(&amp;#39;x&amp;#39;)&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&#34;https://blog.hassaku-labs.com/images/post/hyperopt/hyperopt.png&#34; alt=&#34;hyperopt&#34;&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>インスタンスを表す文字列を分かりやすくする</title>
      <link>https://blog.hassaku-labs.com/post/class-repr/</link>
      <pubDate>Sat, 04 Mar 2017 04:07:38 +0900</pubDate>
      <guid>https://blog.hassaku-labs.com/post/class-repr/</guid>
      <description>&lt;p&gt;ログとかにインスタンスの内容をダンプしたりするとき、つい適当にprintとかlogger.debugしても、&#xA;標準では、インスタンスの内容をスマートに分かりやすく表示してくれたりはしないようです。&lt;/p&gt;&#xA;&lt;p&gt;そのため、デバッグをしやすくするためにも、以下のような感じで__repr__をオーバーライドしておくと、&#xA;分かりやすくなって便利だったりします。&lt;/p&gt;&#xA;&lt;h2 id=&#34;reprを定義しない場合&#34;&gt;reprを定義しない場合&lt;/h2&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;class MyClass(object):&#xA;  def __init__(self):&#xA;    self.attr1 = 123&#xA;    self.attr2 = 456&#xA;&#xA;mc = MyClass()&#xA;print(mc)  # =&amp;gt; &amp;lt;__main__.MyClass object at 0xXXXXXXXXXX&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;reprを定義する場合&#34;&gt;reprを定義する場合&lt;/h2&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;class MyClass(object):&#xA;  def __init__(self):&#xA;    self.attr1 = 123&#xA;    self.attr2 = 456&#xA;&#xA;  def __repr__(self):&#xA;    return &amp;#34;, &amp;#34;.join(&amp;#34;%s: %s&amp;#34; % item for item in vars(self).items())&#xA;&#xA;mc = MyClass()&#xA;print(mc)  # =&amp;gt; attr2: 456, attr1: 123&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;ちょっとしたTipsでした。&lt;/p&gt;</description>
    </item>
    <item>
      <title>ブラウザで動作出来なくなったJavaアプレットをローカルで動かす</title>
      <link>https://blog.hassaku-labs.com/post/run-applet/</link>
      <pubDate>Sun, 26 Feb 2017 03:56:14 +0900</pubDate>
      <guid>https://blog.hassaku-labs.com/post/run-applet/</guid>
      <description>&lt;p&gt;昔作って公開していたJAVAアプレットを発掘したものの、&#xA;ブラウザやJavaのセキュリティ設定をいじっても安定して表示することが出来ませんでした。&#xA;例外サイトとかに追加しても、ネットワーク切り替えなどちょっとしたきっかけで実行不能に&amp;hellip;&lt;/p&gt;&#xA;&lt;p&gt;いろいろ調べた結果、Macの場合、以下のような感じでダウンロードしてから実行することで、&#xA;安定して動作させることが出来ました。&#xA;公開は諦めて、とりあえず身近な人に見せるだけなら、これで良さそうです。&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;$ wget --no-parent -r &amp;#34;javaアプレットのファイル一式が入ったディレクトリXXXXのURL&amp;#34;&#xA;$ cd XXXX&#xA;$ appletviewer XXXX/main.html&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Javaアプレット、一時期はまってたなぁ&amp;hellip;&lt;/p&gt;</description>
    </item>
    <item>
      <title>学習済みword2vecモデルを調べてみた</title>
      <link>https://blog.hassaku-labs.com/post/pretrained-word2vec/</link>
      <pubDate>Sun, 26 Feb 2017 02:27:30 +0900</pubDate>
      <guid>https://blog.hassaku-labs.com/post/pretrained-word2vec/</guid>
      <description>&lt;p&gt;日本語の自然言語処理で分散表現を使おうと思った場合、まず頭に浮かぶのはword2vecだと思います。&#xA;特に分散表現自体の精度とかには興味がなく、それを使った対話システムを作りたいだけだったりするのであれば、&#xA;データクレンジングや学習には結構時間もかかるので、学習済みの公開モデルを使わせていただくのが手っ取り早そうです。&lt;/p&gt;&#xA;&lt;p&gt;(単語ベクトルの準備に手間取り、モチベーション低下に繋がる悪い例：[対話システムを作りたい！【準備編１】]&#xA;(&lt;a href=&#34;http://blog.hassaku-labs.com/post/dialogue_system1/&#34;&gt;http://blog.hassaku-labs.com/post/dialogue_system1/&lt;/a&gt;))&lt;/p&gt;&#xA;&lt;p&gt;調べてみると、よく出来ていそうな公開モデルを２つ見つけたので、その利用方法と気になるベクトル次元数と単語数を調べてみました。&#xA;なお、どちらもWikipedia日本語版を学習元にしているようです。&lt;/p&gt;&#xA;&lt;p&gt;word2vecを使うには、以下のバージョンのgensimを利用します。&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;$ pip freeze | grep gensim&#xA;gensim==1.0.0&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;白ヤギコーポレーションのモデル&#34;&gt;白ヤギコーポレーションのモデル&lt;/h2&gt;&#xA;&lt;p&gt;[word2vecの学習済み日本語モデルを公開します]&#xA;(&lt;a href=&#34;http://aial.shiroyagi.co.jp/2017/02/japanese-word2vec-model-builder/&#34;&gt;http://aial.shiroyagi.co.jp/2017/02/japanese-word2vec-model-builder/&lt;/a&gt;)&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; from gensim.models import word2vec&#xA;&amp;gt;&amp;gt;&amp;gt; model = word2vec.Word2Vec.load(&amp;#39;./shiroyagi/word2vec.gensim.model&amp;#39;)&#xA;&amp;gt;&amp;gt;&amp;gt; model[u&amp;#39;ニュース&amp;#39;].shape&#xA;(50,)&#xA;&amp;gt;&amp;gt;&amp;gt; model.corpus_count&#xA;1046708&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;東北大学-乾岡崎研究室のモデル&#34;&gt;東北大学 乾・岡崎研究室のモデル&lt;/h2&gt;&#xA;&lt;p&gt;[日本語 Wikipedia エンティティベクトル]&#xA;(&lt;a href=&#34;http://www.cl.ecei.tohoku.ac.jp/~m-suzuki/jawiki_vector/&#34;&gt;http://www.cl.ecei.tohoku.ac.jp/~m-suzuki/jawiki_vector/&lt;/a&gt;)&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; from gensim.models import KeyedVectors&#xA;&amp;gt;&amp;gt;&amp;gt; model = KeyedVectors.load_word2vec_format(&amp;#39;./tohoku_entity_vector/entity_vector.model.bin&amp;#39;, binary=True)&#xA;&amp;gt;&amp;gt;&amp;gt; model[u&amp;#39;ニュース&amp;#39;].shape&#xA;(200,)&#xA;&amp;gt;&amp;gt;&amp;gt; len(model.vocab)&#xA;1005367&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;h4 id=&#34;gensimが100未満の場合ただしシロヤギさんのモデルは100以上が必要のようです&#34;&gt;gensimが1.0.0未満の場合（ただしシロヤギさんのモデルは1.0.0以上が必要のようです）&lt;/h4&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; from gensim.models import word2vec&#xA;&amp;gt;&amp;gt;&amp;gt; model = word2vec.Word2Vec.load_word2vec_format(&amp;#39;./tohoku_entity_vector/entity_vector.model.bin&amp;#39;, binary=True)&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;追記facebookの学習済みfasttextモデル&#34;&gt;（追記）Facebookの学習済みFastTextモデル&lt;/h2&gt;&#xA;&lt;p&gt;後日、FacebookのFastTextのレポジトリでも、日本語Wikipediaの分散表現モデルが公開されているのを見つけました。&#xA;ボキャブラリ数は少ないみたいですが、ベクトルの次元数が一番大きいです。&#xA;もしかすると、ボキャブラリ数に関しては、容量の観点から、不要な用語を除去するなど、うまくデータクレンジングされた結果なのかもしれません。&#xA;利用時には要調査です。&lt;/p&gt;&#xA;&lt;p&gt;[Pre-trained word vectors]&#xA;(&lt;a href=&#34;https://github.com/facebookresearch/fastText/blob/master/pretrained-vectors.md&#34;&gt;https://github.com/facebookresearch/fastText/blob/master/pretrained-vectors.md&lt;/a&gt;)&lt;/p&gt;</description>
    </item>
    <item>
      <title>対話システムを作りたい！（実際はWikipediaのデータを取得して単語ベクトルを学習するまで）</title>
      <link>https://blog.hassaku-labs.com/post/dialogue_system1/</link>
      <pubDate>Fri, 01 Apr 2016 02:26:21 +0900</pubDate>
      <guid>https://blog.hassaku-labs.com/post/dialogue_system1/</guid>
      <description>&lt;p&gt;2016年はVRとか流行りそうで、仮想空間での生活を妄想してしまう今日このごろ。&#xA;でも、今のままだとNPCがちゃんと自然に会話してくれない気がして微妙なんですよね。。。そこで、既存技術の延長で、どれくらいの日本語対話が可能か、ちょっと自分でも作ってみたくなりました。&lt;/p&gt;&#xA;&lt;p&gt;自然言語処理をちゃんと勉強したことはないけれど、脳型情報処理アプローチでいくとしたら、結局はベクトル時系列データの処理なのかな？って思います。とりあえず、色々試してみましょう。&lt;/p&gt;&#xA;&lt;p&gt;たぶん、進め方はこんな感じ。&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;単語のベクトルデータ化（言語コーパス？）&lt;/li&gt;&#xA;&lt;li&gt;対話データのベクトル時系列データ化（対話コーパス？）&lt;/li&gt;&#xA;&lt;li&gt;会話時系列データにおける応答時系列データの予測学習&lt;/li&gt;&#xA;&lt;li&gt;学習結果を用いた対話システム構築&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;というわけで、今回は１の言語コーパス作成について。単語を入力とし、N次元ベクトルに変換することを目標。似たような単語は近くに配置されるような変換が好ましい（分散表現だ！）。&lt;/p&gt;&#xA;&lt;h1 id=&#34;言語コーパスをwikipediaの記事から作成&#34;&gt;言語コーパスをWikipediaの記事から作成&lt;/h1&gt;&#xA;&lt;h2 id=&#34;wikipediaの日本語記事をダウンロード&#34;&gt;wikipediaの日本語記事をダウンロード&lt;/h2&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;$ curl -O http://dumps.wikimedia.org/jawiki/latest/jawiki-latest-pages-articles.xml.bz&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;mecabをインストール&#34;&gt;mecabをインストール&lt;/h2&gt;&#xA;&lt;p&gt;単語の分かち書きへ変換するためのツールです。macならbrewでインストール可能。&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;$ brew install mecab&#xA;$ brew install mecab-ipadic&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;新語用辞書をインストール&#34;&gt;新語用辞書をインストール&lt;/h3&gt;&#xA;&lt;p&gt;最近の単語は、brewでインストールされた辞書には含まれていないので、新語に対応した辞書に更新します。&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;$ git clone --depth 1 git@github.com:neologd/mecab-ipadic-neologd.git&#xA;$ cd mecab-ipadic-neologd/&#xA;$ ./bin/install-mecab-ipadic-neologd -n   # 辞書updateも同じコマンド&#xA;$ echo `mecab-config --dicdir`&amp;#34;/mecab-ipadic-neologd&amp;#34;  # 実行時指定のパスを調べる&#xA;/usr/local/lib/mecab/dic/mecab-ipadic-neologd&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;参考新御用辞書有無を確認&#34;&gt;（参考）新御用辞書有無を確認&lt;/h3&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;$ pip install mecab&#xA;$ python&#xA;In [1]: import MeCab&#xA;In [2]: mecab_org = MeCab.Tagger(&amp;#34;-Owakati&amp;#34;)&#xA;In [3]: mecab_new = MeCab.Tagger(&amp;#34;-Owakati -d /usr/local/lib/mecab/dic/mecab-ipadic-neologd&amp;#34;)&#xA;In [4]: print mecab_org.parse(&amp;#34;電力自由化がはじまる&amp;#34;)&#xA;電力 自由 化 が はじまる&#xA;In [5]: print mecab_new.parse(&amp;#34;電力自由化がはじまる&amp;#34;)&#xA;電力自由化 が はじまる&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;なんとなく最近ニュースとかで出てくるような単語を分かち書き出来ている（気がします）。&lt;/p&gt;</description>
    </item>
    <item>
      <title>exception-slackerというライブラリをPyPIに登録した</title>
      <link>https://blog.hassaku-labs.com/post/exception_slacker/</link>
      <pubDate>Fri, 01 Apr 2016 00:35:40 +0900</pubDate>
      <guid>https://blog.hassaku-labs.com/post/exception_slacker/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://github.com/hassaku/exception-slacker&#34;&gt;https://github.com/hassaku/exception-slacker&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;importして、Slackのtokenや投稿チャンネルなどを環境変数でセットしておけば、例外が発生した際に、以下のような感じでSlackへ投稿します。&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;https://blog.hassaku-labs.com/images/post/exception_slacker/exception_slacker.png&#34; alt=&#34;exception_slacker&#34;&gt;&lt;/p&gt;&#xA;&lt;p&gt;時間のかかるプロセスなどを実行している時に、いちいちコンソールをチェックする手間が省けるので、個人的に便利だと思って公開しました。&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
