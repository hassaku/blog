<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Machine Learning on hassaku&#39;s blog</title>
    <link>http://blog.hassaku-labs.com/tags/machine-learning/</link>
    <description>Recent content in Machine Learning on hassaku&#39;s blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>ja</language>
    <lastBuildDate>Sat, 04 Mar 2017 04:08:51 +0900</lastBuildDate>
    
	<atom:link href="http://blog.hassaku-labs.com/tags/machine-learning/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>hyperoptでパラメータサーチ</title>
      <link>http://blog.hassaku-labs.com/post/hyperopt/</link>
      <pubDate>Sat, 04 Mar 2017 04:08:51 +0900</pubDate>
      
      <guid>http://blog.hassaku-labs.com/post/hyperopt/</guid>
      <description>機械学習におけるモデルのハイパーパラメータ探索は、その後のモデル評価を正当に行うことにも繋がる、重要な作業です。 近年ではRandom SearchやGrid Search、GA等の各種最適化手法よりも、色々と優れた手法が提案されているらしく、 手軽に使えるようなライブラリも整備されています。 パラメータ探索技術というと、応用範囲が広く、効果が見えやすいため、手軽に使えて効率的な手法があれば、積極的に使っていきたいところです。 その中でもhyperoptというライブラリが、kaggleとかでよく使われているという話を見かけて、試しに使ってみました。
中身については、色々Blogや論文が見つかるのですが、 Bayesian Optimization -&amp;gt; Sequential Model-based Algorithm Configuration (SMAC) -&amp;gt; Tree-structured Parzen Estimator (TPE) のように進化してきたTPEという手法が使われているようです。 Bayesian Optimizationのアプローチは、直感的にも効率良さそうなので、その進化系なら期待できます。
Hyperopt
簡単な使い方 以下のような感じで、簡単に記述出来て、手軽に取り入れられそうです。
#!/usr/bin/env python # encoding: utf-8 import os import sys import matplotlib.pyplot as plt import numpy as np from hyperopt import fmin, tpe, hp, Trials # パラメータの探索範囲。指定方法は離散値や連続値などに応じて色々ある。 # https://github.com/hyperopt/hyperopt/wiki/FMin#21-parameter-expressions hyperopt_parameters = { &#39;x&#39;: hp.uniform(&#39;x&#39;, -30, 30) } # 最小化したい目的関数。正解との誤差とか。 def objective(args): return np.sin(args[&#39;x&#39;]) / args[&#39;x&#39;] # 探索中の進行状況を記録 trials = Trials() # パラメータサーチの実行 best = fmin(objective, hyperopt_parameters, algo=tpe.</description>
    </item>
    
  </channel>
</rss>