<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Nlp on hassaku&#39;s blog</title>
    <link>http://blog.hassaku-labs.com/tags/nlp/</link>
    <description>Recent content in Nlp on hassaku&#39;s blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>ja</language>
    <lastBuildDate>Sun, 24 Jan 2016 02:26:21 +0900</lastBuildDate>
    <atom:link href="http://blog.hassaku-labs.com/tags/nlp/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>対話システムを作りたい！【準備編１】</title>
      <link>http://blog.hassaku-labs.com/post/dialogue_system1/</link>
      <pubDate>Sun, 24 Jan 2016 02:26:21 +0900</pubDate>
      
      <guid>http://blog.hassaku-labs.com/post/dialogue_system1/</guid>
      <description>

&lt;p&gt;2016年はVRとか流行りそうで、仮想空間での生活を妄想してしまう今日このごろ。
でも、今のままだとNPCがちゃんと自然に会話してくれない気がして微妙なんですよね。。。そこで、既存技術の延長で、どれくらいの日本語対話が可能か、ちょっと自分でも作ってみたくなりました。&lt;/p&gt;

&lt;p&gt;自然言語処理をちゃんと勉強したことはないけれど、脳型情報処理アプローチでいくとしたら、結局はベクトル時系列データの処理なのかな？って思います。とりあえず、色々試してみましょう。&lt;/p&gt;

&lt;p&gt;たぶん、進め方はこんな感じ。&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;単語のベクトルデータ化（言語コーパス？）&lt;/li&gt;
&lt;li&gt;対話データのベクトル時系列データ化（対話コーパス？）&lt;/li&gt;
&lt;li&gt;会話時系列データにおける応答時系列データの予測学習&lt;/li&gt;
&lt;li&gt;学習結果を用いた対話システム構築&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;というわけで、今回は１の言語コーパス作成について。単語を入力とし、N次元ベクトルに変換することを目標。似たような単語は近くに配置されるような変換が好ましい（分散表現だ！）。&lt;/p&gt;

&lt;h1 id=&#34;言語コーパスをwikipediaの記事から作成:9f533d3f859ab3c9a17cb0f76df4e5f1&#34;&gt;言語コーパスをWikipediaの記事から作成&lt;/h1&gt;

&lt;h2 id=&#34;wikipediaの日本語記事をダウンロード:9f533d3f859ab3c9a17cb0f76df4e5f1&#34;&gt;wikipediaの日本語記事をダウンロード&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;$ curl -O http://dumps.wikimedia.org/jawiki/latest/jawiki-latest-pages-articles.xml.bz
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;mecabをインストール:9f533d3f859ab3c9a17cb0f76df4e5f1&#34;&gt;mecabをインストール&lt;/h2&gt;

&lt;p&gt;単語の分かち書きへ変換するためのツールです。macならbrewでインストール可能。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ brew install mecab
$ brew install mecab-ipadic
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;新語用辞書をインストール:9f533d3f859ab3c9a17cb0f76df4e5f1&#34;&gt;新語用辞書をインストール&lt;/h3&gt;

&lt;p&gt;最近の単語は、brewでインストールされた辞書には含まれていないので、新語に対応した辞書に更新します。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ git clone --depth 1 git@github.com:neologd/mecab-ipadic-neologd.git 
$ cd mecab-ipadic-neologd/
$ ./bin/install-mecab-ipadic-neologd -n   # 辞書updateも同じコマンド
$ echo `mecab-config --dicdir`&amp;quot;/mecab-ipadic-neologd&amp;quot;  # 実行時指定のパスを調べる
/usr/local/lib/mecab/dic/mecab-ipadic-neologd
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;参考-新御用辞書有無を確認:9f533d3f859ab3c9a17cb0f76df4e5f1&#34;&gt;（参考）新御用辞書有無を確認&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;$ pip install mecab
$ python
In [1]: import MeCab
In [2]: mecab_org = MeCab.Tagger(&amp;quot;-Owakati&amp;quot;)
In [3]: mecab_new = MeCab.Tagger(&amp;quot;-Owakati -d /usr/local/lib/mecab/dic/mecab-ipadic-neologd&amp;quot;)
In [4]: print mecab_org.parse(&amp;quot;電力自由化がはじまる&amp;quot;)
電力 自由 化 が はじまる 
In [5]: print mecab_new.parse(&amp;quot;電力自由化がはじまる&amp;quot;)
電力自由化 が はじまる 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;なんとなく最近ニュースとかで出てくるような単語を分かち書き出来ている（気がします）。&lt;/p&gt;

&lt;h2 id=&#34;wikipediaを分かち書き:9f533d3f859ab3c9a17cb0f76df4e5f1&#34;&gt;wikipediaを分かち書き&lt;/h2&gt;

&lt;p&gt;各単語に分かち書き変換します。&lt;/p&gt;

&lt;h3 id=&#34;htmlタグとかを取っ払って文章だけに変換:9f533d3f859ab3c9a17cb0f76df4e5f1&#34;&gt;HTMLタグとかを取っ払って文章だけに変換&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;$ echo &#39;gem &amp;quot;wp2txt&amp;quot;&#39; &amp;gt;&amp;gt; Gemfile 
$ bundle
$ bundle exec wp2txt --input-file jawiki-latest-pages-articles.xml.bz
$ ls wp2txt/  # 変換結果
$ rm jawiki-latest-pages-articles.xml.bz # 不要だから消す
$ cat wp2txt/jawiki-latest-pages-articles-* &amp;gt; corpus.txt # 変換結果を一つのファイルに連結
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;とても時間かかります&amp;hellip;
記号除去とかした方が良いのかどうか、今のところ謎です。&lt;/p&gt;

&lt;h3 id=&#34;新語辞書使って分かち書き:9f533d3f859ab3c9a17cb0f76df4e5f1&#34;&gt;新語辞書使って分かち書き&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;$ mecab -Owakati -d /usr/local/lib/mecab/dic/mecab-ipadic-neologd corpus.txt &amp;gt; corpus_wakati.txt
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;とても時間かかります&amp;hellip;&lt;/p&gt;

&lt;h2 id=&#34;word2vecによるベクトル化:9f533d3f859ab3c9a17cb0f76df4e5f1&#34;&gt;word2vecによるベクトル化&lt;/h2&gt;

&lt;p&gt;今後pythonで実装することもあり、gensim使います。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ pip install gensim
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;utf-8に変換:9f533d3f859ab3c9a17cb0f76df4e5f1&#34;&gt;utf-8に変換&lt;/h3&gt;

&lt;p&gt;しないとword2vecのときに、文字コードについて怒られたので&amp;hellip;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ iconv -c -t UTF-8 &amp;lt; corpus_wakati.txt &amp;gt; corpus_wakati_utf-8.txt 
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;学習:9f533d3f859ab3c9a17cb0f76df4e5f1&#34;&gt;学習&lt;/h3&gt;

&lt;p&gt;以下のpythonコードを実行します。次元は適当に中程度としました。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ vi word2vec_train.py
# coding: utf-8
from gensim.models import word2vec
import sys, logging, string, codecs

logging.basicConfig(format=&#39;%(asctime)s : %(levelname)s : %(message)s&#39;, level=logging.INFO)

# 学習（400次元だと４コアで約３時間...）
sentences = word2vec.Text8Corpus(&amp;quot;corpus_wakati_utf-8.txt&amp;quot;)
model = word2vec.Word2Vec(sentences, size=400, workers=4)
# モデルの保存
model.save(&amp;quot;w2v_model_%d_dims&amp;quot; % dims)
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;検証:9f533d3f859ab3c9a17cb0f76df4e5f1&#34;&gt;検証&lt;/h3&gt;

&lt;h4 id=&#34;モデルの読み込み:9f533d3f859ab3c9a17cb0f76df4e5f1&#34;&gt;モデルの読み込み&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;$ pyton
In [1]: from gensim.models import word2vec
In [2]: model = word2vec.Word2Vec.load(&amp;quot;./word2vec_models/w2v_model_%d_dims&amp;quot; % 400)
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;ベクトル空間上で近い単語を探す:9f533d3f859ab3c9a17cb0f76df4e5f1&#34;&gt;ベクトル空間上で近い単語を探す&lt;/h4&gt;

&lt;p&gt;動作確認です。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;In [3]: most_similar = model.most_similar(positive=[u&#39;サッカー&#39;])[0]
In [4]: most_similar[0]
ラグビー
In [5]: most_similar[1]
0.663492918015  # コサイン距離？
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;ベクトル取得:9f533d3f859ab3c9a17cb0f76df4e5f1&#34;&gt;ベクトル取得&lt;/h4&gt;

&lt;p&gt;このベクトルに対して今後処理していくことになります。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;In [1]: vector = model[u&#39;サッカー&#39;]
array([  2.69545317e-01,  -1.99663490e-01,   9.52050760e-02,
         2.16732353e-01,   1.97090670e-01,  -1.90409079e-01,
         ...
In [2]: vector.shape
(400,)
In [3]: vector.min()
-0.71880466
In [4]: vector.max()
0.75658286
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;ベクトルから単語を探す:9f533d3f859ab3c9a17cb0f76df4e5f1&#34;&gt;ベクトルから単語を探す&lt;/h4&gt;

&lt;p&gt;処理結果のベクトルから単語を復元する手段も確認しておきます。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;In [5]: vector[0:10] = 0.0  # 適当に変更  
In [6]: for cname in [candidate[0] for candidate in model.most_similar(positive=[vector], topn=3)]:
            print cname
サッカー
ラグビー
フットサル
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;おわり:9f533d3f859ab3c9a17cb0f76df4e5f1&#34;&gt;おわり&lt;/h2&gt;

&lt;p&gt;ひとまず今回はここまで。単語をベクトル化したことにより、文章をベクトル時系列データとして扱うことが出来、色々な機械学習手法が適用可能になりました。
次回以降色々試してみたいと思います。&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&#34;おまけ:9f533d3f859ab3c9a17cb0f76df4e5f1&#34;&gt;おまけ&lt;/h1&gt;

&lt;p&gt;この時点でも色々作って遊べますね。例えば、以前作った絵文字サジェスト用hubotでは、キーワードに近しい、絵文字キーワードを引っ張ってきて候補を表示してました。Githubとかだと、標準でも絵文字キーワードを入力すると、続きをサジェストしてくれるものの、そもそものキーワードが思いつかなかったりするので、そういうときに便利です :)&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://blog.hassaku-labs.com/images/post/dialogue_system1/word2emoji.jpg&#34; alt=&#34;word2emoji.jpg&#34; /&gt;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>