<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>hassaku&#39;s blog</title>
    <link>http://blog.hassaku-labs.com/</link>
    <description>Recent content on hassaku&#39;s blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>ja</language>
    <lastBuildDate>Sun, 15 Jul 2018 00:00:00 +0900</lastBuildDate>
    
	<atom:link href="http://blog.hassaku-labs.com/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>単語ベクトルと全結合ニューラルネットワークによる単語連想記憶</title>
      <link>http://blog.hassaku-labs.com/post/associative-word-vector/</link>
      <pubDate>Sun, 15 Jul 2018 00:00:00 +0900</pubDate>
      
      <guid>http://blog.hassaku-labs.com/post/associative-word-vector/</guid>
      <description>自然言語処理にニューラルネットワークを適用する事例が増えている。 ここでは、従来の部分的に再帰結合をもつようなRNN(LSTM)といった構造ではなく、 (最近はあまり流行っていない）ホップフィールドモデルのような全結合構造のニューラルネットを用いて、 エネルギーポテンシャルの窪み、引き込み領域を有するアトラクタ空間に言語知識を記憶させることを目指す。
メリットとして、以下のようなことが挙げられる。
 追加学習が容易 データを大量に学習しなくても汎化性能が高い  いくつか想定している内容のうち、今回は単語ベクトルのペアを連想記憶する基本的なタスクを検証する。
検証用コード https://github.com/hassaku/associative-word-vector/tree/blog-1
検証内容 単語ベクトルは、Github上のREADMEに書いてあるように、東北大学 乾・岡崎研究室で公開されている学習済みのものを利用させて頂いた。
タスクについては、以下に示すような「鳩」(que) → 「飛ぶ」(target) といった単純な知識を学習させることにする。
que target 鳩 飛ぶ 牛 走る 鯉 泳ぐ  その後、以下のような未学習の単語(que)に対し、学習済みの知識に基づき、適切な単語(target)を想起出来るかどうかを確かめる。
 que target カラス 飛ぶ 馬 走る 鯛 泳ぐ  期待される結果としては、「カラス」も「鳩」と同じく鳥であることから、「飛ぶ」と連想されること。 原理的には、図のような線上のアトラクタに知識を埋め込むことにより、学習済みのもの(que1)に親しい単語ベクトル(que1&amp;rsquo;)であれば、 引き込まれてtarget1を記憶させた平衡点に至ることで想起が実現される。
今回は原理確認のため、以下のように事前に単語ベクトル間の近さ（コサイン類似度）を確認した上でタスク設定している。
 word カラス 泳ぐ 牛 走る 飛ぶ 馬 鯉 鯛 鳩 カラス 1.000000 0.395338 0.475648 0.162681 0.493993 0.230542 0.406434 0.324465 0.670461 泳ぐ 0.395338 1.000000 0.166645 0.576415 0.</description>
    </item>
    
    <item>
      <title>ターミナル上で１行の簡易グラフ</title>
      <link>http://blog.hassaku-labs.com/post/ascii-simple-plot/</link>
      <pubDate>Mon, 23 Apr 2018 00:00:00 +0900</pubDate>
      
      <guid>http://blog.hassaku-labs.com/post/ascii-simple-plot/</guid>
      <description>データ処理などしていると、処理中の状況を確認するのに、数値や文字列よりもグラフの方が適していることが多い。 ただ、そのためだけにGUIなどを用意するのは大変だし、出来ればターミナル上で表示したい。 というわけで、ログなど混ぜて、以下のような感じで、簡易的にグラフ描くと便利。
$ cat simple_plot.py # coding: utf-8 import numpy as np TICKS = u&#39;_▁▂▃▄▅▆▇█&#39; def ascii_plot(ints, max_range=None, min_range=0, width=40): assert len(ints) &amp;gt;= width ints = np.array(ints, dtype=int) ints = ints[:int(len(ints)/width)*width] ints = np.nansum(np.reshape(ints, (int(len(ints)/width), width)).T, axis=1) if not max_range: max_range = max(ints) if not min_range: min_range = min(ints) step_range = max_range - min_range step = (step_range / float(len(TICKS) - 1)) or 1 return u&#39;&#39;.join(TICKS[int(round((i - min_range) / step))] for i in ints) if __name__ == &amp;quot;__main__&amp;quot;: print(ascii_plot(range(10) + range(10, 0, -1), width=20))  $ python simple_plot.</description>
    </item>
    
    <item>
      <title>pythonの各種使い方をオフラインで確認する方法</title>
      <link>http://blog.hassaku-labs.com/post/python-offline-help/</link>
      <pubDate>Tue, 20 Feb 2018 00:00:00 +0900</pubDate>
      
      <guid>http://blog.hassaku-labs.com/post/python-offline-help/</guid>
      <description>インターネットから隔離されていたりや書籍が持ち込みできないような環境で、プログラミングをすることがあった。 こういうときのために、各種ライブラリの使い方などをオフラインでも確認できるようにしておくと良い。
各種ライブラリのヘルプ確認の仕方 例えば、scikit-learnのkmeansクラスタリングの使い方を調べたいとき。 一番上から順に探していく。
&amp;gt; import sklearn &amp;gt; help(sklearn) ... PACKAGE CONTENTS ... cluster (package) ...  &amp;gt; import sklearn.cluster &amp;gt; help(sklearn.cluster) ... PACKAGE CONTENTS ... k_means_ ...  &amp;gt; import sklearn.cluster.k_means_ &amp;gt; help(sklearn.cluster.k_means_) class KMeans(sklearn.base.BaseEstimator, sklearn.base.ClusterMixin, sklearn.base.TransformerMixin) | K-Means clustering | | Parameters ... 目的の使い方にたどり着いた  jupyter notebook上でのやり方 &amp;gt; import pandas as pd &amp;gt; pd.read_csv?  下部にヘルプ用のウインドウが別途表示されるので、若干見やすい（？）</description>
    </item>
    
    <item>
      <title>Slackのメッセージ収集</title>
      <link>http://blog.hassaku-labs.com/post/slack-mining/</link>
      <pubDate>Tue, 23 Jan 2018 00:00:00 +0900</pubDate>
      
      <guid>http://blog.hassaku-labs.com/post/slack-mining/</guid>
      <description>Slackに投稿されたメッセージを収集する方法についてのメモ
データexport https://xxxx.slack.com/services/export
exportが完了すると、slack上でbotから通知くる
ユーザID取得 $ curl https://slack.com/api/users.list\?token\=YOUR_SLACK_TOKEN  最新のSlack Token取得方法は色々記事が挙がっているのでググること
一ヶ月のユーザの発言を収集する $ cat show_messages.rb require &#39;json&#39; user = &amp;quot;USER_ID&amp;quot; year = 2016 month = 2 days = 29 days.times do |day| date = &amp;quot;%d-%2d-%02d&amp;quot; % (year, month, day + 1) export_file = &amp;quot;./#{date}.json&amp;quot; next unless File.exist?(export_file) puts &amp;quot;\n----- #{date} -----&amp;quot; json_data = open(export_file) do |io| JSON.load(io) end json_data.each do |json| puts &amp;quot;#{Time.at(json[&#39;ts&#39;].to_i)}: #{json[&#39;text&#39;].gsub(/\n+/, &#39; &#39;)}&amp;quot; if json[&amp;quot;user&amp;quot;] == user end end $ ruby show_messages.</description>
    </item>
    
    <item>
      <title>partial AUCでprecision重視の評価</title>
      <link>http://blog.hassaku-labs.com/post/partial-auc/</link>
      <pubDate>Sat, 23 Dec 2017 00:59:39 +0900</pubDate>
      
      <guid>http://blog.hassaku-labs.com/post/partial-auc/</guid>
      <description>機械学習の判別器を評価する際、F値やAUCはよく使われていると思います。 しかしながら、実応用の分野によっては、検出出来ないこと(false negative)よりも、誤検出(false positive)の方が問題視されることがよくあります。 そういったprecision重視にしたいケースで使える指標がpartial AUCです。 一般的には、false positiveに制約をもたせて用いるようです。AUCの面積を求める際に、そのfalse positive rateの下限値以上の部分のみ使います。
簡単に実装するため、scikit learnのaucのコードをオーバーライドして使っています。
import numpy as np from sklearn.metrics import roc_auc_score, roc_curve, auc from sklearn.metrics.base import _average_binary_score # method overriding def roc_auc_score(y_true, y_score, average=&amp;quot;macro&amp;quot;, sample_weight=None, max_fpr=None): def _binary_roc_auc_score(y_true, y_score, sample_weight=None, max_fpr=max_fpr): fpr, tpr, tresholds = roc_curve(y_true, y_score, sample_weight=sample_weight) if max_fpr: idx = np.where(fpr &amp;lt;= max_fpr)[0] idx_last = idx.max() idx_next = idx_last + 1 xc = [fpr[idx_last], fpr[idx_next]] yc = [tpr[idx_last], tpr[idx_next]] tpr = np.</description>
    </item>
    
    <item>
      <title>自分流論文メモのやり方</title>
      <link>http://blog.hassaku-labs.com/post/paper-management/</link>
      <pubDate>Fri, 22 Dec 2017 17:17:30 +0900</pubDate>
      
      <guid>http://blog.hassaku-labs.com/post/paper-management/</guid>
      <description>普段から興味ある分野について、arXivを漁っている人多いと思うのですが、やはり手っ取り早く内容を俯瞰するには、abstractが日本語になっている方が負荷が少ないと思います。日本語の概要を一瞬見て、ちゃんと読んだ方がいいかどうか判断する感じです。
自分は以下のような感じで、arxivのidを貼り付けると、自動でタイトルとか概要を取得して、隣に翻訳も表示してくれるようなスプレッドシートを用意しているのですが、便利なので公開したいと思います。ついでに、実装が公開されていればそのリンクや、簡単な感想なんかもメモするようにしています。
 以下から適当にコピーして頂ければ、使えるかと思います。
https://docs.google.com/spreadsheets/d/19AHvw82bFWivJtoVTohvUDs-uMuhyH0Dw0IzIpvviog
誰かのお役に立てれば幸いです。。</description>
    </item>
    
    <item>
      <title>hyperoptでパラメータサーチ</title>
      <link>http://blog.hassaku-labs.com/post/hyperopt/</link>
      <pubDate>Sat, 04 Mar 2017 04:08:51 +0900</pubDate>
      
      <guid>http://blog.hassaku-labs.com/post/hyperopt/</guid>
      <description>機械学習におけるモデルのハイパーパラメータ探索は、その後のモデル評価を正当に行うことにも繋がる、重要な作業です。 近年ではRandom SearchやGrid Search、GA等の各種最適化手法よりも、色々と優れた手法が提案されているらしく、 手軽に使えるようなライブラリも整備されています。 パラメータ探索技術というと、応用範囲が広く、効果が見えやすいため、手軽に使えて効率的な手法があれば、積極的に使っていきたいところです。 その中でもhyperoptというライブラリが、kaggleとかでよく使われているという話を見かけて、試しに使ってみました。
中身については、色々Blogや論文が見つかるのですが、 Bayesian Optimization -&amp;gt; Sequential Model-based Algorithm Configuration (SMAC) -&amp;gt; Tree-structured Parzen Estimator (TPE) のように進化してきたTPEという手法が使われているようです。 Bayesian Optimizationのアプローチは、直感的にも効率良さそうなので、その進化系なら期待できます。
Hyperopt
簡単な使い方 以下のような感じで、簡単に記述出来て、手軽に取り入れられそうです。
#!/usr/bin/env python # encoding: utf-8 import os import sys import matplotlib.pyplot as plt import numpy as np from hyperopt import fmin, tpe, hp, Trials # パラメータの探索範囲。指定方法は離散値や連続値などに応じて色々ある。 # https://github.com/hyperopt/hyperopt/wiki/FMin#21-parameter-expressions hyperopt_parameters = { &#39;x&#39;: hp.uniform(&#39;x&#39;, -30, 30) } # 最小化したい目的関数。正解との誤差とか。 def objective(args): return np.sin(args[&#39;x&#39;]) / args[&#39;x&#39;] # 探索中の進行状況を記録 trials = Trials() # パラメータサーチの実行 best = fmin(objective, hyperopt_parameters, algo=tpe.</description>
    </item>
    
    <item>
      <title>インスタンスを表す文字列を分かりやすくする</title>
      <link>http://blog.hassaku-labs.com/post/class-repr/</link>
      <pubDate>Sat, 04 Mar 2017 04:07:38 +0900</pubDate>
      
      <guid>http://blog.hassaku-labs.com/post/class-repr/</guid>
      <description>ログとかにインスタンスの内容をダンプしたりするとき、つい適当にprintとかlogger.debugしても、 標準では、インスタンスの内容をスマートに分かりやすく表示してくれたりはしないようです。
そのため、デバッグをしやすくするためにも、以下のような感じで__repr__をオーバーライドしておくと、 分かりやすくなって便利だったりします。
reprを定義しない場合 class MyClass(object): def __init__(self): self.attr1 = 123 self.attr2 = 456 mc = MyClass() print(mc) # =&amp;gt; &amp;lt;__main__.MyClass object at 0xXXXXXXXXXX&amp;gt;  reprを定義する場合 class MyClass(object): def __init__(self): self.attr1 = 123 self.attr2 = 456 def __repr__(self): return &amp;quot;, &amp;quot;.join(&amp;quot;%s: %s&amp;quot; % item for item in vars(self).items()) mc = MyClass() print(mc) # =&amp;gt; attr2: 456, attr1: 123  ちょっとしたTipsでした。</description>
    </item>
    
    <item>
      <title>ブラウザで動作出来なくなったJavaアプレットをローカルで動かす</title>
      <link>http://blog.hassaku-labs.com/post/run-applet/</link>
      <pubDate>Sun, 26 Feb 2017 03:56:14 +0900</pubDate>
      
      <guid>http://blog.hassaku-labs.com/post/run-applet/</guid>
      <description>昔作って公開していたJAVAアプレットを発掘したものの、 ブラウザやJavaのセキュリティ設定をいじっても安定して表示することが出来ませんでした。 例外サイトとかに追加しても、ネットワーク切り替えなどちょっとしたきっかけで実行不能に&amp;hellip;
いろいろ調べた結果、Macの場合、以下のような感じでダウンロードしてから実行することで、 安定して動作させることが出来ました。 公開は諦めて、とりあえず身近な人に見せるだけなら、これで良さそうです。
$ wget --no-parent -r &amp;quot;javaアプレットのファイル一式が入ったディレクトリXXXXのURL&amp;quot; $ cd XXXX $ appletviewer XXXX/main.html  Javaアプレット、一時期はまってたなぁ&amp;hellip;</description>
    </item>
    
    <item>
      <title>学習済みword2vecモデルを調べてみた</title>
      <link>http://blog.hassaku-labs.com/post/pretrained-word2vec/</link>
      <pubDate>Sun, 26 Feb 2017 02:27:30 +0900</pubDate>
      
      <guid>http://blog.hassaku-labs.com/post/pretrained-word2vec/</guid>
      <description>日本語の自然言語処理で分散表現を使おうと思った場合、まず頭に浮かぶのはword2vecだと思います。 特に分散表現自体の精度とかには興味がなく、それを使った対話システムを作りたいだけだったりするのであれば、 データクレンジングや学習には結構時間もかかるので、学習済みの公開モデルを使わせていただくのが手っ取り早そうです。
(単語ベクトルの準備に手間取り、モチベーション低下に繋がる悪い例：対話システムを作りたい！【準備編１】)
調べてみると、よく出来ていそうな公開モデルを２つ見つけたので、その利用方法と気になるベクトル次元数と単語数を調べてみました。 なお、どちらもWikipedia日本語版を学習元にしているようです。
word2vecを使うには、以下のバージョンのgensimを利用します。
$ pip freeze | grep gensim gensim==1.0.0  白ヤギコーポレーションのモデル word2vecの学習済み日本語モデルを公開します
&amp;gt;&amp;gt;&amp;gt; from gensim.models import word2vec &amp;gt;&amp;gt;&amp;gt; model = word2vec.Word2Vec.load(&#39;./shiroyagi/word2vec.gensim.model&#39;) &amp;gt;&amp;gt;&amp;gt; model[u&#39;ニュース&#39;].shape (50,) &amp;gt;&amp;gt;&amp;gt; model.corpus_count 1046708  東北大学 乾・岡崎研究室のモデル 日本語 Wikipedia エンティティベクトル
&amp;gt;&amp;gt;&amp;gt; from gensim.models import KeyedVectors &amp;gt;&amp;gt;&amp;gt; model = KeyedVectors.load_word2vec_format(&#39;./tohoku_entity_vector/entity_vector.model.bin&#39;, binary=True) &amp;gt;&amp;gt;&amp;gt; model[u&#39;ニュース&#39;].shape (200,) &amp;gt;&amp;gt;&amp;gt; len(model.vocab) 1005367  gensimが1.0.0未満の場合（ただしシロヤギさんのモデルは1.0.0以上が必要のようです） &amp;gt;&amp;gt;&amp;gt; from gensim.models import word2vec &amp;gt;&amp;gt;&amp;gt; model = word2vec.Word2Vec.load_word2vec_format(&#39;./tohoku_entity_vector/entity_vector.model.bin&#39;, binary=True)  （追記）Facebookの学習済みFastTextモデル 後日、FacebookのFastTextのレポジトリでも、日本語Wikipediaの分散表現モデルが公開されているのを見つけました。 ボキャブラリ数は少ないみたいですが、ベクトルの次元数が一番大きいです。 もしかすると、ボキャブラリ数に関しては、容量の観点から、不要な用語を除去するなど、うまくデータクレンジングされた結果なのかもしれません。 利用時には要調査です。</description>
    </item>
    
    <item>
      <title>対話システムを作りたい！（実際はWikipediaのデータを取得して単語ベクトルを学習するまで）</title>
      <link>http://blog.hassaku-labs.com/post/dialogue_system1/</link>
      <pubDate>Fri, 01 Apr 2016 02:26:21 +0900</pubDate>
      
      <guid>http://blog.hassaku-labs.com/post/dialogue_system1/</guid>
      <description>2016年はVRとか流行りそうで、仮想空間での生活を妄想してしまう今日このごろ。 でも、今のままだとNPCがちゃんと自然に会話してくれない気がして微妙なんですよね。。。そこで、既存技術の延長で、どれくらいの日本語対話が可能か、ちょっと自分でも作ってみたくなりました。
自然言語処理をちゃんと勉強したことはないけれど、脳型情報処理アプローチでいくとしたら、結局はベクトル時系列データの処理なのかな？って思います。とりあえず、色々試してみましょう。
たぶん、進め方はこんな感じ。
 単語のベクトルデータ化（言語コーパス？） 対話データのベクトル時系列データ化（対話コーパス？） 会話時系列データにおける応答時系列データの予測学習 学習結果を用いた対話システム構築  というわけで、今回は１の言語コーパス作成について。単語を入力とし、N次元ベクトルに変換することを目標。似たような単語は近くに配置されるような変換が好ましい（分散表現だ！）。
言語コーパスをWikipediaの記事から作成 wikipediaの日本語記事をダウンロード $ curl -O http://dumps.wikimedia.org/jawiki/latest/jawiki-latest-pages-articles.xml.bz  mecabをインストール 単語の分かち書きへ変換するためのツールです。macならbrewでインストール可能。
$ brew install mecab $ brew install mecab-ipadic  新語用辞書をインストール 最近の単語は、brewでインストールされた辞書には含まれていないので、新語に対応した辞書に更新します。
$ git clone --depth 1 git@github.com:neologd/mecab-ipadic-neologd.git $ cd mecab-ipadic-neologd/ $ ./bin/install-mecab-ipadic-neologd -n # 辞書updateも同じコマンド $ echo `mecab-config --dicdir`&amp;quot;/mecab-ipadic-neologd&amp;quot; # 実行時指定のパスを調べる /usr/local/lib/mecab/dic/mecab-ipadic-neologd  （参考）新御用辞書有無を確認 $ pip install mecab $ python In [1]: import MeCab In [2]: mecab_org = MeCab.Tagger(&amp;quot;-Owakati&amp;quot;) In [3]: mecab_new = MeCab.</description>
    </item>
    
    <item>
      <title>exception-slackerというライブラリをPyPIに登録した</title>
      <link>http://blog.hassaku-labs.com/post/exception_slacker/</link>
      <pubDate>Fri, 01 Apr 2016 00:35:40 +0900</pubDate>
      
      <guid>http://blog.hassaku-labs.com/post/exception_slacker/</guid>
      <description>https://github.com/hassaku/exception-slacker
importして、Slackのtokenや投稿チャンネルなどを環境変数でセットしておけば、例外が発生した際に、以下のような感じでSlackへ投稿します。
時間のかかるプロセスなどを実行している時に、いちいちコンソールをチェックする手間が省けるので、個人的に便利だと思って公開しました。</description>
    </item>
    
  </channel>
</rss>