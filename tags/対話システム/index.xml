<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>対話システム on hassaku&#39;s blog</title>
    <link>https://blog.hassaku-labs.com/tags/%E5%AF%BE%E8%A9%B1%E3%82%B7%E3%82%B9%E3%83%86%E3%83%A0/</link>
    <description>Recent content in 対話システム on hassaku&#39;s blog</description>
    <generator>Hugo</generator>
    <language>ja</language>
    <lastBuildDate>Sun, 26 Feb 2017 02:27:30 +0900</lastBuildDate>
    <atom:link href="https://blog.hassaku-labs.com/tags/%E5%AF%BE%E8%A9%B1%E3%82%B7%E3%82%B9%E3%83%86%E3%83%A0/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>学習済みword2vecモデルを調べてみた</title>
      <link>https://blog.hassaku-labs.com/post/pretrained-word2vec/</link>
      <pubDate>Sun, 26 Feb 2017 02:27:30 +0900</pubDate>
      <guid>https://blog.hassaku-labs.com/post/pretrained-word2vec/</guid>
      <description>&lt;p&gt;日本語の自然言語処理で分散表現を使おうと思った場合、まず頭に浮かぶのはword2vecだと思います。&#xA;特に分散表現自体の精度とかには興味がなく、それを使った対話システムを作りたいだけだったりするのであれば、&#xA;データクレンジングや学習には結構時間もかかるので、学習済みの公開モデルを使わせていただくのが手っ取り早そうです。&lt;/p&gt;&#xA;&lt;p&gt;(単語ベクトルの準備に手間取り、モチベーション低下に繋がる悪い例：[対話システムを作りたい！【準備編１】]&#xA;(&lt;a href=&#34;http://blog.hassaku-labs.com/post/dialogue_system1/&#34;&gt;http://blog.hassaku-labs.com/post/dialogue_system1/&lt;/a&gt;))&lt;/p&gt;&#xA;&lt;p&gt;調べてみると、よく出来ていそうな公開モデルを２つ見つけたので、その利用方法と気になるベクトル次元数と単語数を調べてみました。&#xA;なお、どちらもWikipedia日本語版を学習元にしているようです。&lt;/p&gt;&#xA;&lt;p&gt;word2vecを使うには、以下のバージョンのgensimを利用します。&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;$ pip freeze | grep gensim&#xA;gensim==1.0.0&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;白ヤギコーポレーションのモデル&#34;&gt;白ヤギコーポレーションのモデル&lt;/h2&gt;&#xA;&lt;p&gt;[word2vecの学習済み日本語モデルを公開します]&#xA;(&lt;a href=&#34;http://aial.shiroyagi.co.jp/2017/02/japanese-word2vec-model-builder/&#34;&gt;http://aial.shiroyagi.co.jp/2017/02/japanese-word2vec-model-builder/&lt;/a&gt;)&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; from gensim.models import word2vec&#xA;&amp;gt;&amp;gt;&amp;gt; model = word2vec.Word2Vec.load(&amp;#39;./shiroyagi/word2vec.gensim.model&amp;#39;)&#xA;&amp;gt;&amp;gt;&amp;gt; model[u&amp;#39;ニュース&amp;#39;].shape&#xA;(50,)&#xA;&amp;gt;&amp;gt;&amp;gt; model.corpus_count&#xA;1046708&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;東北大学-乾岡崎研究室のモデル&#34;&gt;東北大学 乾・岡崎研究室のモデル&lt;/h2&gt;&#xA;&lt;p&gt;[日本語 Wikipedia エンティティベクトル]&#xA;(&lt;a href=&#34;http://www.cl.ecei.tohoku.ac.jp/~m-suzuki/jawiki_vector/&#34;&gt;http://www.cl.ecei.tohoku.ac.jp/~m-suzuki/jawiki_vector/&lt;/a&gt;)&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; from gensim.models import KeyedVectors&#xA;&amp;gt;&amp;gt;&amp;gt; model = KeyedVectors.load_word2vec_format(&amp;#39;./tohoku_entity_vector/entity_vector.model.bin&amp;#39;, binary=True)&#xA;&amp;gt;&amp;gt;&amp;gt; model[u&amp;#39;ニュース&amp;#39;].shape&#xA;(200,)&#xA;&amp;gt;&amp;gt;&amp;gt; len(model.vocab)&#xA;1005367&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;h4 id=&#34;gensimが100未満の場合ただしシロヤギさんのモデルは100以上が必要のようです&#34;&gt;gensimが1.0.0未満の場合（ただしシロヤギさんのモデルは1.0.0以上が必要のようです）&lt;/h4&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; from gensim.models import word2vec&#xA;&amp;gt;&amp;gt;&amp;gt; model = word2vec.Word2Vec.load_word2vec_format(&amp;#39;./tohoku_entity_vector/entity_vector.model.bin&amp;#39;, binary=True)&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;追記facebookの学習済みfasttextモデル&#34;&gt;（追記）Facebookの学習済みFastTextモデル&lt;/h2&gt;&#xA;&lt;p&gt;後日、FacebookのFastTextのレポジトリでも、日本語Wikipediaの分散表現モデルが公開されているのを見つけました。&#xA;ボキャブラリ数は少ないみたいですが、ベクトルの次元数が一番大きいです。&#xA;もしかすると、ボキャブラリ数に関しては、容量の観点から、不要な用語を除去するなど、うまくデータクレンジングされた結果なのかもしれません。&#xA;利用時には要調査です。&lt;/p&gt;&#xA;&lt;p&gt;[Pre-trained word vectors]&#xA;(&lt;a href=&#34;https://github.com/facebookresearch/fastText/blob/master/pretrained-vectors.md&#34;&gt;https://github.com/facebookresearch/fastText/blob/master/pretrained-vectors.md&lt;/a&gt;)&lt;/p&gt;</description>
    </item>
    <item>
      <title>対話システムを作りたい！（実際はWikipediaのデータを取得して単語ベクトルを学習するまで）</title>
      <link>https://blog.hassaku-labs.com/post/dialogue_system1/</link>
      <pubDate>Fri, 01 Apr 2016 02:26:21 +0900</pubDate>
      <guid>https://blog.hassaku-labs.com/post/dialogue_system1/</guid>
      <description>&lt;p&gt;2016年はVRとか流行りそうで、仮想空間での生活を妄想してしまう今日このごろ。&#xA;でも、今のままだとNPCがちゃんと自然に会話してくれない気がして微妙なんですよね。。。そこで、既存技術の延長で、どれくらいの日本語対話が可能か、ちょっと自分でも作ってみたくなりました。&lt;/p&gt;&#xA;&lt;p&gt;自然言語処理をちゃんと勉強したことはないけれど、脳型情報処理アプローチでいくとしたら、結局はベクトル時系列データの処理なのかな？って思います。とりあえず、色々試してみましょう。&lt;/p&gt;&#xA;&lt;p&gt;たぶん、進め方はこんな感じ。&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;単語のベクトルデータ化（言語コーパス？）&lt;/li&gt;&#xA;&lt;li&gt;対話データのベクトル時系列データ化（対話コーパス？）&lt;/li&gt;&#xA;&lt;li&gt;会話時系列データにおける応答時系列データの予測学習&lt;/li&gt;&#xA;&lt;li&gt;学習結果を用いた対話システム構築&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;というわけで、今回は１の言語コーパス作成について。単語を入力とし、N次元ベクトルに変換することを目標。似たような単語は近くに配置されるような変換が好ましい（分散表現だ！）。&lt;/p&gt;&#xA;&lt;h1 id=&#34;言語コーパスをwikipediaの記事から作成&#34;&gt;言語コーパスをWikipediaの記事から作成&lt;/h1&gt;&#xA;&lt;h2 id=&#34;wikipediaの日本語記事をダウンロード&#34;&gt;wikipediaの日本語記事をダウンロード&lt;/h2&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;$ curl -O http://dumps.wikimedia.org/jawiki/latest/jawiki-latest-pages-articles.xml.bz&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;mecabをインストール&#34;&gt;mecabをインストール&lt;/h2&gt;&#xA;&lt;p&gt;単語の分かち書きへ変換するためのツールです。macならbrewでインストール可能。&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;$ brew install mecab&#xA;$ brew install mecab-ipadic&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;新語用辞書をインストール&#34;&gt;新語用辞書をインストール&lt;/h3&gt;&#xA;&lt;p&gt;最近の単語は、brewでインストールされた辞書には含まれていないので、新語に対応した辞書に更新します。&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;$ git clone --depth 1 git@github.com:neologd/mecab-ipadic-neologd.git&#xA;$ cd mecab-ipadic-neologd/&#xA;$ ./bin/install-mecab-ipadic-neologd -n   # 辞書updateも同じコマンド&#xA;$ echo `mecab-config --dicdir`&amp;#34;/mecab-ipadic-neologd&amp;#34;  # 実行時指定のパスを調べる&#xA;/usr/local/lib/mecab/dic/mecab-ipadic-neologd&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;参考新御用辞書有無を確認&#34;&gt;（参考）新御用辞書有無を確認&lt;/h3&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;$ pip install mecab&#xA;$ python&#xA;In [1]: import MeCab&#xA;In [2]: mecab_org = MeCab.Tagger(&amp;#34;-Owakati&amp;#34;)&#xA;In [3]: mecab_new = MeCab.Tagger(&amp;#34;-Owakati -d /usr/local/lib/mecab/dic/mecab-ipadic-neologd&amp;#34;)&#xA;In [4]: print mecab_org.parse(&amp;#34;電力自由化がはじまる&amp;#34;)&#xA;電力 自由 化 が はじまる&#xA;In [5]: print mecab_new.parse(&amp;#34;電力自由化がはじまる&amp;#34;)&#xA;電力自由化 が はじまる&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;なんとなく最近ニュースとかで出てくるような単語を分かち書き出来ている（気がします）。&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
